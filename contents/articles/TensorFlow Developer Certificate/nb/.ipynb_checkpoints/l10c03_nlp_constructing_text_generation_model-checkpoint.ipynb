{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "olNBiJHdSZDe"
   },
   "source": [
    "# Constructing a Text Generation Model\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "RHWdoyRESb1s"
   },
   "source": [
    "Using most of the techniques you've already learned, it's now possible to generate new text by predicting the next word that follows a given seed word. To practice this method, we'll use the [Kaggle Song Lyrics Dataset](https://www.kaggle.com/mousehead/songlyrics)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DqdPMv5MSeL_"
   },
   "source": [
    "## Import TensorFlow and related functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1-aMMAHOSe_V"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# Other imports for processing data\n",
    "import string\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "cIeMYRqjSiGn"
   },
   "source": [
    "## Get the Dataset\n",
    "\n",
    "As noted above, we'll utilize the [Song Lyrics dataset](https://www.kaggle.com/mousehead/songlyrics) on Kaggle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 326
    },
    "colab_type": "code",
    "id": "O2saQAf9Sfs0",
    "outputId": "25146b9b-93ed-415f-dfff-eb73189643d9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2020-08-09 03:38:04--  https://drive.google.com/uc?id=1LiJFZd41ofrWoBtW-pMYsfz1w8Ny0Bj8\n",
      "Resolving drive.google.com (drive.google.com)... 74.125.203.102, 74.125.203.100, 74.125.203.113, ...\n",
      "Connecting to drive.google.com (drive.google.com)|74.125.203.102|:443... connected.\n",
      "HTTP request sent, awaiting response... 302 Moved Temporarily\n",
      "Location: https://doc-04-ak-docs.googleusercontent.com/docs/securesc/ha0ro937gcuc7l7deffksulhg5h7mbp1/knnc0v7obic9cfo7di4dtovne8vb8cr9/1596944250000/11118900490791463723/*/1LiJFZd41ofrWoBtW-pMYsfz1w8Ny0Bj8 [following]\n",
      "Warning: wildcards not supported in HTTP.\n",
      "--2020-08-09 03:38:07--  https://doc-04-ak-docs.googleusercontent.com/docs/securesc/ha0ro937gcuc7l7deffksulhg5h7mbp1/knnc0v7obic9cfo7di4dtovne8vb8cr9/1596944250000/11118900490791463723/*/1LiJFZd41ofrWoBtW-pMYsfz1w8Ny0Bj8\n",
      "Resolving doc-04-ak-docs.googleusercontent.com (doc-04-ak-docs.googleusercontent.com)... 74.125.203.132, 2404:6800:4008:c03::84\n",
      "Connecting to doc-04-ak-docs.googleusercontent.com (doc-04-ak-docs.googleusercontent.com)|74.125.203.132|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: unspecified [text/csv]\n",
      "Saving to: ‘/tmp/songdata.csv’\n",
      "\n",
      "/tmp/songdata.csv       [     <=>            ]  69.08M  56.9MB/s    in 1.2s    \n",
      "\n",
      "2020-08-09 03:38:09 (56.9 MB/s) - ‘/tmp/songdata.csv’ saved [72436445]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget --no-check-certificate \\\n",
    "    https://drive.google.com/uc?id=1LiJFZd41ofrWoBtW-pMYsfz1w8Ny0Bj8 \\\n",
    "    -O /tmp/songdata.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "O_4hKeN0SmjL"
   },
   "source": [
    "## **First 10 Songs**\n",
    "\n",
    "Let's first look at just 10 songs from the dataset, and see how things perform."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "U-Jm2tO_Soua"
   },
   "source": [
    "### Preprocessing\n",
    "\n",
    "Let's perform some basic preprocessing to get rid of punctuation and make everything lowercase. We'll then split the lyrics up by line and tokenize the lyrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wWYHWsdISfpd"
   },
   "outputs": [],
   "source": [
    "def tokenize_corpus(corpus, num_words=-1):\n",
    "  # Fit a Tokenizer on the corpus\n",
    "  if num_words > -1:\n",
    "    tokenizer = Tokenizer(num_words=num_words)\n",
    "  else:\n",
    "    tokenizer = Tokenizer()\n",
    "  tokenizer.fit_on_texts(corpus)\n",
    "  return tokenizer\n",
    "\n",
    "def create_lyrics_corpus(dataset, field):\n",
    "  # Remove all other punctuation\n",
    "  dataset[field] = dataset[field].str.replace('[{}]'.format(string.punctuation), '')\n",
    "  # Make it lowercase\n",
    "  dataset[field] = dataset[field].str.lower()\n",
    "  # Make it one long string to split by line\n",
    "  lyrics = dataset[field].str.cat()\n",
    "  corpus = lyrics.split('\\n')\n",
    "  # Remove any trailing whitespace\n",
    "  for l in range(len(corpus)):\n",
    "    corpus[l] = corpus[l].rstrip()\n",
    "  # Remove any empty lines\n",
    "  corpus = [l for l in corpus if l != '']\n",
    "\n",
    "  return corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 71
    },
    "colab_type": "code",
    "id": "_hEKnlPbSfmp",
    "outputId": "e78f0b25-f458-4876-bec1-a4e82558e9ca"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'you': 1, 'i': 2, 'and': 3, 'a': 4, 'me': 5, 'the': 6, 'is': 7, 'my': 8, 'to': 9, 'ma': 10, 'it': 11, 'of': 12, 'im': 13, 'your': 14, 'love': 15, 'so': 16, 'as': 17, 'that': 18, 'in': 19, 'andante': 20, 'boomaboomerang': 21, 'make': 22, 'on': 23, 'oh': 24, 'for': 25, 'but': 26, 'new': 27, 'bang': 28, 'its': 29, 'be': 30, 'like': 31, 'know': 32, 'now': 33, 'how': 34, 'could': 35, 'youre': 36, 'sing': 37, 'never': 38, 'no': 39, 'chiquitita': 40, 'can': 41, 'we': 42, 'song': 43, 'had': 44, 'good': 45, 'youll': 46, 'she': 47, 'just': 48, 'girl': 49, 'again': 50, 'will': 51, 'take': 52, 'please': 53, 'let': 54, 'am': 55, 'eyes': 56, 'was': 57, 'always': 58, 'cassandra': 59, 'blue': 60, 'time': 61, 'dont': 62, 'were': 63, 'return': 64, 'once': 65, 'then': 66, 'sorry': 67, 'cryin': 68, 'over': 69, 'feel': 70, 'ever': 71, 'believe': 72, 'what': 73, 'do': 74, 'go': 75, 'all': 76, 'out': 77, 'think': 78, 'every': 79, 'leave': 80, 'look': 81, 'at': 82, 'way': 83, 'one': 84, 'music': 85, 'down': 86, 'our': 87, 'give': 88, 'learn': 89, 'more': 90, 'us': 91, 'would': 92, 'there': 93, 'before': 94, 'when': 95, 'with': 96, 'feeling': 97, 'play': 98, 'cause': 99, 'away': 100, 'here': 101, 'have': 102, 'yes': 103, 'baby': 104, 'get': 105, 'didnt': 106, 'see': 107, 'did': 108, 'closed': 109, 'realized': 110, 'crazy': 111, 'world': 112, 'lord': 113, 'shes': 114, 'kind': 115, 'without': 116, 'if': 117, 'touch': 118, 'strong': 119, 'making': 120, 'such': 121, 'found': 122, 'true': 123, 'stay': 124, 'together': 125, 'thought': 126, 'come': 127, 'they': 128, 'sweet': 129, 'tender': 130, 'sender': 131, 'tune': 132, 'humdehumhum': 133, 'gonna': 134, 'last': 135, 'leaving': 136, 'sleep': 137, 'only': 138, 'saw': 139, 'tell': 140, 'hes': 141, 'her': 142, 'sound': 143, 'tread': 144, 'lightly': 145, 'ground': 146, 'ill': 147, 'show': 148, 'life': 149, 'too': 150, 'used': 151, 'darling': 152, 'meant': 153, 'break': 154, 'end': 155, 'yourself': 156, 'little': 157, 'dumbedumdum': 158, 'bedumbedumdum': 159, 'youve': 160, 'dumbbedumbdumb': 161, 'bedumbbedumbdumb': 162, 'by': 163, 'theyre': 164, 'alone': 165, 'misunderstood': 166, 'day': 167, 'dawning': 168, 'some': 169, 'wanted': 170, 'none': 171, 'listen': 172, 'words': 173, 'warning': 174, 'darkest': 175, 'nights': 176, 'nobody': 177, 'knew': 178, 'fight': 179, 'caught': 180, 'really': 181, 'power': 182, 'dreams': 183, 'weave': 184, 'until': 185, 'final': 186, 'hour': 187, 'morning': 188, 'ship': 189, 'gone': 190, 'grieving': 191, 'still': 192, 'pain': 193, 'cry': 194, 'sun': 195, 'try': 196, 'face': 197, 'something': 198, 'sees': 199, 'makes': 200, 'fine': 201, 'who': 202, 'mine': 203, 'leaves': 204, 'walk': 205, 'hand': 206, 'well': 207, 'about': 208, 'things': 209, 'slow': 210, 'theres': 211, 'talk': 212, 'why': 213, 'up': 214, 'lousy': 215, 'packing': 216, 'ive': 217, 'gotta': 218, 'near': 219, 'keeping': 220, 'intention': 221, 'growing': 222, 'taking': 223, 'dimension': 224, 'even': 225, 'better': 226, 'thank': 227, 'god': 228, 'not': 229, 'somebody': 230, 'happy': 231, 'question': 232, 'smile': 233, 'mean': 234, 'much': 235, 'kisses': 236, 'around': 237, 'anywhere': 238, 'advice': 239, 'care': 240, 'use': 241, 'selfish': 242, 'tool': 243, 'fool': 244, 'showing': 245, 'boomerang': 246, 'throwing': 247, 'warm': 248, 'kiss': 249, 'surrender': 250, 'giving': 251, 'been': 252, 'door': 253, 'burning': 254, 'bridges': 255, 'being': 256, 'moving': 257, 'though': 258, 'behind': 259, 'are': 260, 'must': 261, 'sure': 262, 'stood': 263, 'hope': 264, 'this': 265, 'deny': 266, 'sad': 267, 'quiet': 268, 'truth': 269, 'heartaches': 270, 'scars': 271, 'dancing': 272, 'sky': 273, 'shining': 274, 'above': 275, 'hear': 276, 'came': 277, 'couldnt': 278, 'everything': 279, 'back': 280, 'long': 281, 'waitin': 282, 'cold': 283, 'chills': 284, 'bone': 285, 'youd': 286, 'wonderful': 287, 'means': 288, 'special': 289, 'smiles': 290, 'lucky': 291, 'fellow': 292, 'park': 293, 'holds': 294, 'squeezes': 295, 'walking': 296, 'hours': 297, 'talking': 298, 'plan': 299, 'easy': 300, 'gently': 301, 'summer': 302, 'evening': 303, 'breeze': 304, 'grow': 305, 'fingers': 306, 'soft': 307, 'light': 308, 'body': 309, 'velvet': 310, 'night': 311, 'soul': 312, 'slowly': 313, 'shimmer': 314, 'thousand': 315, 'butterflies': 316, 'float': 317, 'put': 318, 'rotten': 319, 'boy': 320, 'tough': 321, 'stuff': 322, 'saying': 323, 'need': 324, 'anymore': 325, 'enough': 326, 'standing': 327, 'creep': 328, 'felt': 329, 'cheap': 330, 'notion': 331, 'deep': 332, 'dumb': 333, 'mistake': 334, 'entitled': 335, 'another': 336, 'beg': 337, 'forgive': 338, 'an': 339, 'feels': 340, 'hoot': 341, 'holler': 342, 'mad': 343, 'under': 344, 'heel': 345, 'holy': 346, 'christ': 347, 'deal': 348, 'sick': 349, 'tired': 350, 'tedious': 351, 'ways': 352, 'aint': 353, 'walkin': 354, 'cutting': 355, 'tie': 356, 'wanna': 357, 'into': 358, 'eye': 359, 'myself': 360, 'counting': 361, 'pride': 362, 'unright': 363, 'neighbours': 364, 'ride': 365, 'burying': 366, 'past': 367, 'peace': 368, 'free': 369, 'sucker': 370, 'street': 371, 'singing': 372, 'shouting': 373, 'staying': 374, 'alive': 375, 'city': 376, 'dead': 377, 'hiding': 378, 'their': 379, 'shame': 380, 'hollow': 381, 'laughter': 382, 'while': 383, 'crying': 384, 'bed': 385, 'pity': 386, 'believed': 387, 'lost': 388, 'from': 389, 'start': 390, 'suffer': 391, 'sell': 392, 'secrets': 393, 'bargain': 394, 'playing': 395, 'smart': 396, 'aching': 397, 'hearts': 398, 'sailing': 399, 'father': 400, 'sister': 401, 'reason': 402, 'linger': 403, 'deeply': 404, 'future': 405, 'casting': 406, 'shadow': 407, 'else': 408, 'fate': 409, 'bags': 410, 'thorough': 411, 'knowing': 412, 'late': 413, 'wait': 414, 'watched': 415, 'harbor': 416, 'sunrise': 417, 'sails': 418, 'almost': 419, 'slack': 420, 'cool': 421, 'rain': 422, 'deck': 423, 'tiny': 424, 'figure': 425, 'rigid': 426, 'restrained': 427, 'filled': 428, 'whats': 429, 'wrong': 430, 'enchained': 431, 'own': 432, 'sorrow': 433, 'tomorrow': 434, 'hate': 435, 'shoulder': 436, 'best': 437, 'friend': 438, 'rely': 439, 'broken': 440, 'feather': 441, 'patch': 442, 'walls': 443, 'tumbling': 444, 'loves': 445, 'blown': 446, 'candle': 447, 'seems': 448, 'hard': 449, 'handle': 450, 'id': 451, 'thinking': 452, 'went': 453, 'house': 454, 'hardly': 455, 'guy': 456, 'closing': 457, 'front': 458, 'emptiness': 459, 'he': 460, 'disapeared': 461, 'his': 462, 'car': 463, 'stunned': 464, 'dreamed': 465, 'lifes': 466, 'part': 467, 'move': 468, 'feet': 469, 'pavement': 470, 'acted': 471, 'told': 472, 'lies': 473, 'meet': 474, 'other': 475, 'guys': 476, 'stupid': 477, 'blind': 478, 'smiled': 479, 'took': 480, 'said': 481, 'may': 482, 'couple': 483, 'men': 484, 'them': 485, 'brother': 486, 'joe': 487, 'seeing': 488, 'lot': 489, 'him': 490, 'nice': 491, 'sitting': 492, 'sittin': 493, 'memories': 494}\n",
      "495\n"
     ]
    }
   ],
   "source": [
    "# Read the dataset from csv - just first 10 songs for now\n",
    "dataset = pd.read_csv('/tmp/songdata.csv', dtype=str)[:10]\n",
    "# Create the corpus using the 'text' column containing lyrics\n",
    "corpus = create_lyrics_corpus(dataset, 'text')\n",
    "# Tokenize the corpus\n",
    "tokenizer = tokenize_corpus(corpus)\n",
    "\n",
    "total_words = len(tokenizer.word_index) + 1\n",
    "\n",
    "print(tokenizer.word_index)\n",
    "print(total_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "yGIh4CczSuOH"
   },
   "source": [
    "### Create Sequences and Labels\n",
    "\n",
    "After preprocessing, we next need to create sequences and labels. Creating the sequences themselves is similar to before with `texts_to_sequences`, but also including the use of [N-Grams](https://towardsdatascience.com/introduction-to-language-models-n-gram-e323081503d9); creating the labels will now utilize those sequences as well as utilize one-hot encoding over all potential output words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qhoKS1TESfkL"
   },
   "outputs": [],
   "source": [
    "sequences = []\n",
    "for line in corpus:\n",
    "\ttoken_list = tokenizer.texts_to_sequences([line])[0]\n",
    "\tfor i in range(1, len(token_list)):\n",
    "\t\tn_gram_sequence = token_list[:i+1]\n",
    "\t\tsequences.append(n_gram_sequence)\n",
    "\n",
    "# Pad sequences for equal input length \n",
    "max_sequence_len = max([len(seq) for seq in sequences])\n",
    "sequences = np.array(pad_sequences(sequences, maxlen=max_sequence_len, padding='pre'))\n",
    "\n",
    "# Split sequences between the \"input\" sequence and \"output\" predicted word\n",
    "input_sequences, labels = sequences[:,:-1], sequences[:,-1]\n",
    "# One-hot encode the labels\n",
    "one_hot_labels = tf.keras.utils.to_categorical(labels, num_classes=total_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 833
    },
    "colab_type": "code",
    "id": "gu82a0TLSfhk",
    "outputId": "89449ccb-6518-4d57-f85f-bfadffcb4b99"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32\n",
      "97\n",
      "[  0   0   0   0   0   0   0   0   0   0   0   0   0  81  82 142 197  29\n",
      "   4]\n",
      "[  0   0   0   0   0   0   0   0   0   0   0   0  81  82 142 197  29   4\n",
      " 287]\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "# Check out how some of our data is being stored\n",
    "# The Tokenizer has just a single index per word\n",
    "print(tokenizer.word_index['know'])\n",
    "print(tokenizer.word_index['feeling'])\n",
    "# Input sequences will have multiple indexes\n",
    "print(input_sequences[5])\n",
    "print(input_sequences[6])\n",
    "# And the one hot labels will be as long as the full spread of tokenized words\n",
    "print(one_hot_labels[5])\n",
    "print(one_hot_labels[6])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xtNshIaNSydO"
   },
   "source": [
    "### Train a Text Generation Model\n",
    "\n",
    "Building an RNN to train our text generation model will be very similar to the sentiment models you've built previously. The only real change necessary is to make sure to use Categorical instead of Binary Cross Entropy as the loss function - we could use Binary before since the sentiment was only 0 or 1, but now there are hundreds of categories.\n",
    "\n",
    "From there, we should also consider using *more* epochs than before, as text generation can take a little longer to converge than sentiment analysis, *and* we aren't working with all that much data yet. I'll set it at 200 epochs here since we're only use part of the dataset, and training will tail off quite a bit over that many epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "JMTGu1qbSfe9",
    "outputId": "4a7a23ad-fe38-4f17-ab07-9c77ed0cb63e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "62/62 [==============================] - 1s 12ms/step - loss: 5.9724 - accuracy: 0.0242\n",
      "Epoch 2/200\n",
      "62/62 [==============================] - 1s 12ms/step - loss: 5.4292 - accuracy: 0.0394\n",
      "Epoch 3/200\n",
      "62/62 [==============================] - 1s 12ms/step - loss: 5.3681 - accuracy: 0.0399\n",
      "Epoch 4/200\n",
      "62/62 [==============================] - 1s 12ms/step - loss: 5.3081 - accuracy: 0.0404\n",
      "Epoch 5/200\n",
      "62/62 [==============================] - 1s 12ms/step - loss: 5.2372 - accuracy: 0.0424\n",
      "Epoch 6/200\n",
      "62/62 [==============================] - 1s 12ms/step - loss: 5.1663 - accuracy: 0.0449\n",
      "Epoch 7/200\n",
      "62/62 [==============================] - 1s 12ms/step - loss: 5.1005 - accuracy: 0.0484\n",
      "Epoch 8/200\n",
      "62/62 [==============================] - 1s 12ms/step - loss: 5.0319 - accuracy: 0.0520\n",
      "Epoch 9/200\n",
      "62/62 [==============================] - 1s 12ms/step - loss: 4.9532 - accuracy: 0.0681\n",
      "Epoch 10/200\n",
      "62/62 [==============================] - 1s 12ms/step - loss: 4.8623 - accuracy: 0.0858\n",
      "Epoch 11/200\n",
      "62/62 [==============================] - 1s 12ms/step - loss: 4.7598 - accuracy: 0.0943\n",
      "Epoch 12/200\n",
      "62/62 [==============================] - 1s 12ms/step - loss: 4.6497 - accuracy: 0.1070\n",
      "Epoch 13/200\n",
      "62/62 [==============================] - 1s 12ms/step - loss: 4.5489 - accuracy: 0.1201\n",
      "Epoch 14/200\n",
      "62/62 [==============================] - 1s 12ms/step - loss: 4.4575 - accuracy: 0.1256\n",
      "Epoch 15/200\n",
      "62/62 [==============================] - 1s 12ms/step - loss: 4.3593 - accuracy: 0.1327\n",
      "Epoch 16/200\n",
      "62/62 [==============================] - 1s 12ms/step - loss: 4.2700 - accuracy: 0.1524\n",
      "Epoch 17/200\n",
      "62/62 [==============================] - 1s 12ms/step - loss: 4.1736 - accuracy: 0.1549\n",
      "Epoch 18/200\n",
      "62/62 [==============================] - 1s 12ms/step - loss: 4.0863 - accuracy: 0.1690\n",
      "Epoch 19/200\n",
      "62/62 [==============================] - 1s 12ms/step - loss: 4.0035 - accuracy: 0.1826\n",
      "Epoch 20/200\n",
      "62/62 [==============================] - 1s 12ms/step - loss: 3.9241 - accuracy: 0.1922\n",
      "Epoch 21/200\n",
      "62/62 [==============================] - 1s 12ms/step - loss: 3.8481 - accuracy: 0.2074\n",
      "Epoch 22/200\n",
      "62/62 [==============================] - 1s 12ms/step - loss: 3.7718 - accuracy: 0.2154\n",
      "Epoch 23/200\n",
      "62/62 [==============================] - 1s 12ms/step - loss: 3.6924 - accuracy: 0.2371\n",
      "Epoch 24/200\n",
      "62/62 [==============================] - 1s 12ms/step - loss: 3.6175 - accuracy: 0.2523\n",
      "Epoch 25/200\n",
      "62/62 [==============================] - 1s 12ms/step - loss: 3.5545 - accuracy: 0.2659\n",
      "Epoch 26/200\n",
      "62/62 [==============================] - 1s 12ms/step - loss: 3.4843 - accuracy: 0.2785\n",
      "Epoch 27/200\n",
      "62/62 [==============================] - 1s 12ms/step - loss: 3.4280 - accuracy: 0.2856\n",
      "Epoch 28/200\n",
      "62/62 [==============================] - 1s 12ms/step - loss: 3.3448 - accuracy: 0.3158\n",
      "Epoch 29/200\n",
      "62/62 [==============================] - 1s 12ms/step - loss: 3.2783 - accuracy: 0.3204\n",
      "Epoch 30/200\n",
      "62/62 [==============================] - 1s 12ms/step - loss: 3.2234 - accuracy: 0.3305\n",
      "Epoch 31/200\n",
      "62/62 [==============================] - 1s 12ms/step - loss: 3.1559 - accuracy: 0.3380\n",
      "Epoch 32/200\n",
      "62/62 [==============================] - 1s 12ms/step - loss: 3.1083 - accuracy: 0.3532\n",
      "Epoch 33/200\n",
      "62/62 [==============================] - 1s 12ms/step - loss: 3.0483 - accuracy: 0.3602\n",
      "Epoch 34/200\n",
      "62/62 [==============================] - 1s 12ms/step - loss: 2.9765 - accuracy: 0.3870\n",
      "Epoch 35/200\n",
      "62/62 [==============================] - 1s 12ms/step - loss: 2.9178 - accuracy: 0.3915\n",
      "Epoch 36/200\n",
      "62/62 [==============================] - 1s 12ms/step - loss: 2.8648 - accuracy: 0.4011\n",
      "Epoch 37/200\n",
      "62/62 [==============================] - 1s 12ms/step - loss: 2.8182 - accuracy: 0.4117\n",
      "Epoch 38/200\n",
      "62/62 [==============================] - 1s 12ms/step - loss: 2.7906 - accuracy: 0.4198\n",
      "Epoch 39/200\n",
      "62/62 [==============================] - 1s 12ms/step - loss: 2.7176 - accuracy: 0.4374\n",
      "Epoch 40/200\n",
      "62/62 [==============================] - 1s 12ms/step - loss: 2.6564 - accuracy: 0.4480\n",
      "Epoch 41/200\n",
      "62/62 [==============================] - 1s 12ms/step - loss: 2.6083 - accuracy: 0.4536\n",
      "Epoch 42/200\n",
      "62/62 [==============================] - 1s 12ms/step - loss: 2.5767 - accuracy: 0.4652\n",
      "Epoch 43/200\n",
      "62/62 [==============================] - 1s 12ms/step - loss: 2.5161 - accuracy: 0.4743\n",
      "Epoch 44/200\n",
      "62/62 [==============================] - 1s 12ms/step - loss: 2.4689 - accuracy: 0.4854\n",
      "Epoch 45/200\n",
      "62/62 [==============================] - 1s 12ms/step - loss: 2.4165 - accuracy: 0.4919\n",
      "Epoch 46/200\n",
      "62/62 [==============================] - 1s 12ms/step - loss: 2.3789 - accuracy: 0.4980\n",
      "Epoch 47/200\n",
      "62/62 [==============================] - 1s 12ms/step - loss: 2.3292 - accuracy: 0.5050\n",
      "Epoch 48/200\n",
      "62/62 [==============================] - 1s 12ms/step - loss: 2.3068 - accuracy: 0.5111\n",
      "Epoch 49/200\n",
      "62/62 [==============================] - 1s 12ms/step - loss: 2.2623 - accuracy: 0.5227\n",
      "Epoch 50/200\n",
      "62/62 [==============================] - 1s 12ms/step - loss: 2.2117 - accuracy: 0.5328\n",
      "Epoch 51/200\n",
      "62/62 [==============================] - 1s 12ms/step - loss: 2.1699 - accuracy: 0.5348\n",
      "Epoch 52/200\n",
      "62/62 [==============================] - 1s 12ms/step - loss: 2.1316 - accuracy: 0.5515\n",
      "Epoch 53/200\n",
      "62/62 [==============================] - 1s 12ms/step - loss: 2.0936 - accuracy: 0.5590\n",
      "Epoch 54/200\n",
      "62/62 [==============================] - 1s 12ms/step - loss: 2.0573 - accuracy: 0.5651\n",
      "Epoch 55/200\n",
      "62/62 [==============================] - 1s 12ms/step - loss: 2.0290 - accuracy: 0.5827\n",
      "Epoch 56/200\n",
      "62/62 [==============================] - 1s 12ms/step - loss: 2.0133 - accuracy: 0.5767\n",
      "Epoch 57/200\n",
      "62/62 [==============================] - 1s 12ms/step - loss: 1.9877 - accuracy: 0.5918\n",
      "Epoch 58/200\n",
      "62/62 [==============================] - 1s 12ms/step - loss: 1.9610 - accuracy: 0.5898\n",
      "Epoch 59/200\n",
      "62/62 [==============================] - 1s 12ms/step - loss: 1.9115 - accuracy: 0.5984\n",
      "Epoch 60/200\n",
      "62/62 [==============================] - 1s 12ms/step - loss: 1.8776 - accuracy: 0.6039\n",
      "Epoch 61/200\n",
      "62/62 [==============================] - 1s 12ms/step - loss: 1.8356 - accuracy: 0.6160\n",
      "Epoch 62/200\n",
      "62/62 [==============================] - 1s 12ms/step - loss: 1.8023 - accuracy: 0.6206\n",
      "Epoch 63/200\n",
      "62/62 [==============================] - 1s 12ms/step - loss: 1.7700 - accuracy: 0.6282\n",
      "Epoch 64/200\n",
      "62/62 [==============================] - 1s 12ms/step - loss: 1.7376 - accuracy: 0.6367\n",
      "Epoch 65/200\n",
      "62/62 [==============================] - 1s 12ms/step - loss: 1.7119 - accuracy: 0.6418\n",
      "Epoch 66/200\n",
      "62/62 [==============================] - 1s 12ms/step - loss: 1.6993 - accuracy: 0.6478\n",
      "Epoch 67/200\n",
      "62/62 [==============================] - 1s 12ms/step - loss: 1.6875 - accuracy: 0.6453\n",
      "Epoch 68/200\n",
      "62/62 [==============================] - 1s 12ms/step - loss: 1.6407 - accuracy: 0.6514\n",
      "Epoch 69/200\n",
      "62/62 [==============================] - 1s 12ms/step - loss: 1.6228 - accuracy: 0.6635\n",
      "Epoch 70/200\n",
      "62/62 [==============================] - 1s 12ms/step - loss: 1.5839 - accuracy: 0.6710\n",
      "Epoch 71/200\n",
      "62/62 [==============================] - 1s 12ms/step - loss: 1.5651 - accuracy: 0.6705\n",
      "Epoch 72/200\n",
      "62/62 [==============================] - 1s 12ms/step - loss: 1.5358 - accuracy: 0.6776\n",
      "Epoch 73/200\n",
      "62/62 [==============================] - 1s 12ms/step - loss: 1.4952 - accuracy: 0.6867\n",
      "Epoch 74/200\n",
      "62/62 [==============================] - 1s 12ms/step - loss: 1.4604 - accuracy: 0.6958\n",
      "Epoch 75/200\n",
      "62/62 [==============================] - 1s 12ms/step - loss: 1.4427 - accuracy: 0.6988\n",
      "Epoch 76/200\n",
      "62/62 [==============================] - 1s 12ms/step - loss: 1.4148 - accuracy: 0.7149\n",
      "Epoch 77/200\n",
      "62/62 [==============================] - 1s 12ms/step - loss: 1.3930 - accuracy: 0.7170\n",
      "Epoch 78/200\n",
      "62/62 [==============================] - 1s 12ms/step - loss: 1.3732 - accuracy: 0.7190\n",
      "Epoch 79/200\n",
      "62/62 [==============================] - 1s 12ms/step - loss: 1.3470 - accuracy: 0.7245\n",
      "Epoch 80/200\n",
      "62/62 [==============================] - 1s 12ms/step - loss: 1.3257 - accuracy: 0.7316\n",
      "Epoch 81/200\n",
      "62/62 [==============================] - 1s 12ms/step - loss: 1.3140 - accuracy: 0.7331\n",
      "Epoch 82/200\n",
      "62/62 [==============================] - 1s 12ms/step - loss: 1.3094 - accuracy: 0.7311\n",
      "Epoch 83/200\n",
      "62/62 [==============================] - 1s 12ms/step - loss: 1.2939 - accuracy: 0.7306\n",
      "Epoch 84/200\n",
      "62/62 [==============================] - 1s 12ms/step - loss: 1.2621 - accuracy: 0.7422\n",
      "Epoch 85/200\n",
      "62/62 [==============================] - 1s 12ms/step - loss: 1.2546 - accuracy: 0.7472\n",
      "Epoch 86/200\n",
      "62/62 [==============================] - 1s 12ms/step - loss: 1.2130 - accuracy: 0.7533\n",
      "Epoch 87/200\n",
      "62/62 [==============================] - 1s 12ms/step - loss: 1.1923 - accuracy: 0.7608\n",
      "Epoch 88/200\n",
      "62/62 [==============================] - 1s 12ms/step - loss: 1.1650 - accuracy: 0.7659\n",
      "Epoch 89/200\n",
      "62/62 [==============================] - 1s 12ms/step - loss: 1.1537 - accuracy: 0.7694\n",
      "Epoch 90/200\n",
      "62/62 [==============================] - 1s 12ms/step - loss: 1.1534 - accuracy: 0.7619\n",
      "Epoch 91/200\n",
      "62/62 [==============================] - 1s 12ms/step - loss: 1.1250 - accuracy: 0.7770\n",
      "Epoch 92/200\n",
      "62/62 [==============================] - 1s 12ms/step - loss: 1.1160 - accuracy: 0.7770\n",
      "Epoch 93/200\n",
      "62/62 [==============================] - 1s 12ms/step - loss: 1.0920 - accuracy: 0.7790\n",
      "Epoch 94/200\n",
      "62/62 [==============================] - 1s 12ms/step - loss: 1.0797 - accuracy: 0.7825\n",
      "Epoch 95/200\n",
      "62/62 [==============================] - 1s 12ms/step - loss: 1.0744 - accuracy: 0.7810\n",
      "Epoch 96/200\n",
      "62/62 [==============================] - 1s 12ms/step - loss: 1.0604 - accuracy: 0.7820\n",
      "Epoch 97/200\n",
      "62/62 [==============================] - 1s 12ms/step - loss: 1.1200 - accuracy: 0.7634\n",
      "Epoch 98/200\n",
      "62/62 [==============================] - 1s 12ms/step - loss: 1.1580 - accuracy: 0.7548\n",
      "Epoch 99/200\n",
      "62/62 [==============================] - 1s 12ms/step - loss: 1.1155 - accuracy: 0.7654\n",
      "Epoch 100/200\n",
      "62/62 [==============================] - 1s 12ms/step - loss: 1.0531 - accuracy: 0.7881\n",
      "Epoch 101/200\n",
      "62/62 [==============================] - 1s 12ms/step - loss: 1.0029 - accuracy: 0.7962\n",
      "Epoch 102/200\n",
      "62/62 [==============================] - 1s 12ms/step - loss: 0.9886 - accuracy: 0.7997\n",
      "Epoch 103/200\n",
      "62/62 [==============================] - 1s 12ms/step - loss: 0.9667 - accuracy: 0.8037\n",
      "Epoch 104/200\n",
      "62/62 [==============================] - 1s 12ms/step - loss: 0.9546 - accuracy: 0.8113\n",
      "Epoch 105/200\n",
      "62/62 [==============================] - 1s 12ms/step - loss: 0.9346 - accuracy: 0.8174\n",
      "Epoch 106/200\n",
      "62/62 [==============================] - 1s 12ms/step - loss: 0.9238 - accuracy: 0.8148\n",
      "Epoch 107/200\n",
      "62/62 [==============================] - 1s 12ms/step - loss: 0.9068 - accuracy: 0.8179\n",
      "Epoch 108/200\n",
      "62/62 [==============================] - 1s 12ms/step - loss: 0.8920 - accuracy: 0.8194\n",
      "Epoch 109/200\n",
      "62/62 [==============================] - 1s 12ms/step - loss: 0.8814 - accuracy: 0.8280\n",
      "Epoch 110/200\n",
      "62/62 [==============================] - 1s 12ms/step - loss: 0.8691 - accuracy: 0.8214\n",
      "Epoch 111/200\n",
      "62/62 [==============================] - 1s 12ms/step - loss: 0.8644 - accuracy: 0.8290\n",
      "Epoch 112/200\n",
      "62/62 [==============================] - 1s 12ms/step - loss: 0.8514 - accuracy: 0.8234\n",
      "Epoch 113/200\n",
      "62/62 [==============================] - 1s 12ms/step - loss: 0.8606 - accuracy: 0.8224\n",
      "Epoch 114/200\n",
      "62/62 [==============================] - 1s 12ms/step - loss: 0.9316 - accuracy: 0.7982\n",
      "Epoch 115/200\n",
      "62/62 [==============================] - 1s 12ms/step - loss: 0.8746 - accuracy: 0.8174\n",
      "Epoch 116/200\n",
      "62/62 [==============================] - 1s 12ms/step - loss: 0.8515 - accuracy: 0.8184\n",
      "Epoch 117/200\n",
      "62/62 [==============================] - 1s 12ms/step - loss: 0.8235 - accuracy: 0.8219\n",
      "Epoch 118/200\n",
      "62/62 [==============================] - 1s 12ms/step - loss: 0.8081 - accuracy: 0.8310\n",
      "Epoch 119/200\n",
      "62/62 [==============================] - 1s 12ms/step - loss: 0.8113 - accuracy: 0.8345\n",
      "Epoch 120/200\n",
      "62/62 [==============================] - 1s 12ms/step - loss: 0.7916 - accuracy: 0.8375\n",
      "Epoch 121/200\n",
      "62/62 [==============================] - 1s 12ms/step - loss: 0.7771 - accuracy: 0.8446\n",
      "Epoch 122/200\n",
      "62/62 [==============================] - 1s 12ms/step - loss: 0.7633 - accuracy: 0.8406\n",
      "Epoch 123/200\n",
      "62/62 [==============================] - 1s 12ms/step - loss: 0.7582 - accuracy: 0.8370\n",
      "Epoch 124/200\n",
      "62/62 [==============================] - 1s 12ms/step - loss: 0.7574 - accuracy: 0.8416\n",
      "Epoch 125/200\n",
      "62/62 [==============================] - 1s 12ms/step - loss: 0.7398 - accuracy: 0.8486\n",
      "Epoch 126/200\n",
      "62/62 [==============================] - 1s 12ms/step - loss: 0.7237 - accuracy: 0.8507\n",
      "Epoch 127/200\n",
      "62/62 [==============================] - 1s 12ms/step - loss: 0.7091 - accuracy: 0.8502\n",
      "Epoch 128/200\n",
      "62/62 [==============================] - 1s 12ms/step - loss: 0.7094 - accuracy: 0.8522\n",
      "Epoch 129/200\n",
      "62/62 [==============================] - 1s 12ms/step - loss: 0.6952 - accuracy: 0.8562\n",
      "Epoch 130/200\n",
      "62/62 [==============================] - 1s 12ms/step - loss: 0.6905 - accuracy: 0.8592\n",
      "Epoch 131/200\n",
      "62/62 [==============================] - 1s 12ms/step - loss: 0.6781 - accuracy: 0.8557\n",
      "Epoch 132/200\n",
      "62/62 [==============================] - 1s 12ms/step - loss: 0.6728 - accuracy: 0.8557\n",
      "Epoch 133/200\n",
      "62/62 [==============================] - 1s 12ms/step - loss: 0.6657 - accuracy: 0.8587\n",
      "Epoch 134/200\n",
      "62/62 [==============================] - 1s 12ms/step - loss: 0.6545 - accuracy: 0.8602\n",
      "Epoch 135/200\n",
      "62/62 [==============================] - 1s 12ms/step - loss: 0.6460 - accuracy: 0.8673\n",
      "Epoch 136/200\n",
      "62/62 [==============================] - 1s 12ms/step - loss: 0.6413 - accuracy: 0.8613\n",
      "Epoch 137/200\n",
      "62/62 [==============================] - 1s 12ms/step - loss: 0.6381 - accuracy: 0.8663\n",
      "Epoch 138/200\n",
      "62/62 [==============================] - 1s 12ms/step - loss: 0.6286 - accuracy: 0.8628\n",
      "Epoch 139/200\n",
      "62/62 [==============================] - 1s 12ms/step - loss: 0.6525 - accuracy: 0.8562\n",
      "Epoch 140/200\n",
      "62/62 [==============================] - 1s 12ms/step - loss: 0.6521 - accuracy: 0.8562\n",
      "Epoch 141/200\n",
      "62/62 [==============================] - 1s 12ms/step - loss: 0.6365 - accuracy: 0.8592\n",
      "Epoch 142/200\n",
      "62/62 [==============================] - 1s 12ms/step - loss: 0.6118 - accuracy: 0.8658\n",
      "Epoch 143/200\n",
      "62/62 [==============================] - 1s 12ms/step - loss: 0.6262 - accuracy: 0.8602\n",
      "Epoch 144/200\n",
      "62/62 [==============================] - 1s 12ms/step - loss: 0.6330 - accuracy: 0.8577\n",
      "Epoch 145/200\n",
      "62/62 [==============================] - 1s 12ms/step - loss: 0.6116 - accuracy: 0.8653\n",
      "Epoch 146/200\n",
      "62/62 [==============================] - 1s 12ms/step - loss: 0.5963 - accuracy: 0.8668\n",
      "Epoch 147/200\n",
      "62/62 [==============================] - 1s 12ms/step - loss: 0.5905 - accuracy: 0.8713\n",
      "Epoch 148/200\n",
      "62/62 [==============================] - 1s 12ms/step - loss: 0.5796 - accuracy: 0.8729\n",
      "Epoch 149/200\n",
      "62/62 [==============================] - 1s 12ms/step - loss: 0.5802 - accuracy: 0.8713\n",
      "Epoch 150/200\n",
      "62/62 [==============================] - 1s 12ms/step - loss: 0.5791 - accuracy: 0.8729\n",
      "Epoch 151/200\n",
      "62/62 [==============================] - 1s 12ms/step - loss: 0.5686 - accuracy: 0.8744\n",
      "Epoch 152/200\n",
      "62/62 [==============================] - 1s 12ms/step - loss: 0.5535 - accuracy: 0.8794\n",
      "Epoch 153/200\n",
      "62/62 [==============================] - 1s 12ms/step - loss: 0.5459 - accuracy: 0.8779\n",
      "Epoch 154/200\n",
      "62/62 [==============================] - 1s 12ms/step - loss: 0.5383 - accuracy: 0.8769\n",
      "Epoch 155/200\n",
      "62/62 [==============================] - 1s 12ms/step - loss: 0.5316 - accuracy: 0.8804\n",
      "Epoch 156/200\n",
      "62/62 [==============================] - 1s 12ms/step - loss: 0.5265 - accuracy: 0.8829\n",
      "Epoch 157/200\n",
      "62/62 [==============================] - 1s 12ms/step - loss: 0.5200 - accuracy: 0.8779\n",
      "Epoch 158/200\n",
      "62/62 [==============================] - 1s 12ms/step - loss: 0.5170 - accuracy: 0.8809\n",
      "Epoch 159/200\n",
      "62/62 [==============================] - 1s 12ms/step - loss: 0.5527 - accuracy: 0.8698\n",
      "Epoch 160/200\n",
      "62/62 [==============================] - 1s 12ms/step - loss: 0.5365 - accuracy: 0.8749\n",
      "Epoch 161/200\n",
      "62/62 [==============================] - 1s 12ms/step - loss: 0.5134 - accuracy: 0.8850\n",
      "Epoch 162/200\n",
      "62/62 [==============================] - 1s 12ms/step - loss: 0.5071 - accuracy: 0.8789\n",
      "Epoch 163/200\n",
      "62/62 [==============================] - 1s 12ms/step - loss: 0.5030 - accuracy: 0.8774\n",
      "Epoch 164/200\n",
      "62/62 [==============================] - 1s 12ms/step - loss: 0.5444 - accuracy: 0.8749\n",
      "Epoch 165/200\n",
      "62/62 [==============================] - 1s 12ms/step - loss: 0.5229 - accuracy: 0.8804\n",
      "Epoch 166/200\n",
      "62/62 [==============================] - 1s 12ms/step - loss: 0.5140 - accuracy: 0.8784\n",
      "Epoch 167/200\n",
      "62/62 [==============================] - 1s 12ms/step - loss: 0.5084 - accuracy: 0.8759\n",
      "Epoch 168/200\n",
      "62/62 [==============================] - 1s 12ms/step - loss: 0.4933 - accuracy: 0.8789\n",
      "Epoch 169/200\n",
      "62/62 [==============================] - 1s 12ms/step - loss: 0.4867 - accuracy: 0.8829\n",
      "Epoch 170/200\n",
      "62/62 [==============================] - 1s 12ms/step - loss: 0.4805 - accuracy: 0.8845\n",
      "Epoch 171/200\n",
      "62/62 [==============================] - 1s 12ms/step - loss: 0.4684 - accuracy: 0.8860\n",
      "Epoch 172/200\n",
      "62/62 [==============================] - 1s 12ms/step - loss: 0.4813 - accuracy: 0.8880\n",
      "Epoch 173/200\n",
      "62/62 [==============================] - 1s 12ms/step - loss: 0.4685 - accuracy: 0.8840\n",
      "Epoch 174/200\n",
      "62/62 [==============================] - 1s 12ms/step - loss: 0.4555 - accuracy: 0.8895\n",
      "Epoch 175/200\n",
      "62/62 [==============================] - 1s 12ms/step - loss: 0.4523 - accuracy: 0.8865\n",
      "Epoch 176/200\n",
      "62/62 [==============================] - 1s 12ms/step - loss: 0.4599 - accuracy: 0.8865\n",
      "Epoch 177/200\n",
      "62/62 [==============================] - 1s 12ms/step - loss: 0.4589 - accuracy: 0.8865\n",
      "Epoch 178/200\n",
      "62/62 [==============================] - 1s 12ms/step - loss: 0.4471 - accuracy: 0.8875\n",
      "Epoch 179/200\n",
      "62/62 [==============================] - 1s 12ms/step - loss: 0.4813 - accuracy: 0.8794\n",
      "Epoch 180/200\n",
      "62/62 [==============================] - 1s 12ms/step - loss: 0.4968 - accuracy: 0.8764\n",
      "Epoch 181/200\n",
      "62/62 [==============================] - 1s 12ms/step - loss: 0.4731 - accuracy: 0.8774\n",
      "Epoch 182/200\n",
      "62/62 [==============================] - 1s 12ms/step - loss: 0.4611 - accuracy: 0.8819\n",
      "Epoch 183/200\n",
      "62/62 [==============================] - 1s 12ms/step - loss: 0.4384 - accuracy: 0.8865\n",
      "Epoch 184/200\n",
      "62/62 [==============================] - 1s 12ms/step - loss: 0.4295 - accuracy: 0.8930\n",
      "Epoch 185/200\n",
      "62/62 [==============================] - 1s 12ms/step - loss: 0.4191 - accuracy: 0.8935\n",
      "Epoch 186/200\n",
      "62/62 [==============================] - 1s 12ms/step - loss: 0.4165 - accuracy: 0.8905\n",
      "Epoch 187/200\n",
      "62/62 [==============================] - 1s 12ms/step - loss: 0.4147 - accuracy: 0.8930\n",
      "Epoch 188/200\n",
      "62/62 [==============================] - 1s 12ms/step - loss: 0.4408 - accuracy: 0.8850\n",
      "Epoch 189/200\n",
      "62/62 [==============================] - 1s 12ms/step - loss: 0.4187 - accuracy: 0.8905\n",
      "Epoch 190/200\n",
      "62/62 [==============================] - 1s 12ms/step - loss: 0.4117 - accuracy: 0.8920\n",
      "Epoch 191/200\n",
      "62/62 [==============================] - 1s 12ms/step - loss: 0.4080 - accuracy: 0.8961\n",
      "Epoch 192/200\n",
      "62/62 [==============================] - 1s 12ms/step - loss: 0.4059 - accuracy: 0.8935\n",
      "Epoch 193/200\n",
      "62/62 [==============================] - 1s 12ms/step - loss: 0.3991 - accuracy: 0.8961\n",
      "Epoch 194/200\n",
      "62/62 [==============================] - 1s 12ms/step - loss: 0.3939 - accuracy: 0.8971\n",
      "Epoch 195/200\n",
      "62/62 [==============================] - 1s 12ms/step - loss: 0.3893 - accuracy: 0.8951\n",
      "Epoch 196/200\n",
      "62/62 [==============================] - 1s 12ms/step - loss: 0.3852 - accuracy: 0.9011\n",
      "Epoch 197/200\n",
      "62/62 [==============================] - 1s 12ms/step - loss: 0.3817 - accuracy: 0.9011\n",
      "Epoch 198/200\n",
      "62/62 [==============================] - 1s 12ms/step - loss: 0.3774 - accuracy: 0.8991\n",
      "Epoch 199/200\n",
      "62/62 [==============================] - 1s 12ms/step - loss: 0.3776 - accuracy: 0.9041\n",
      "Epoch 200/200\n",
      "62/62 [==============================] - 1s 12ms/step - loss: 0.3785 - accuracy: 0.8966\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense, Bidirectional\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(total_words, 64, input_length=max_sequence_len-1))\n",
    "model.add(Bidirectional(LSTM(20)))\n",
    "model.add(Dense(total_words, activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "history = model.fit(input_sequences, one_hot_labels, epochs=200, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "uLObc_UuS1p4"
   },
   "source": [
    "### View the Training Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 279
    },
    "colab_type": "code",
    "id": "7tyRUQLqSfcE",
    "outputId": "ae4c8b3f-6847-4547-885f-764c1ab1b6ec"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light",
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_graphs(history, string):\n",
    "  plt.plot(history.history[string])\n",
    "  plt.xlabel(\"Epochs\")\n",
    "  plt.ylabel(string)\n",
    "  plt.show()\n",
    "\n",
    "plot_graphs(history, 'accuracy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6x-ncfl_S4Qa"
   },
   "source": [
    "### Generate new lyrics!\n",
    "\n",
    "It's finally time to generate some new lyrics from the trained model, and see what we get. To do so, we'll provide some \"seed text\", or an input sequence for the model to start with. We'll also decide just how long of an output sequence we want - this could essentially be infinite, as the input plus the previous output will be continuously fed in for a new output word (at least up to our max sequence length)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "colab_type": "code",
    "id": "ucEsfYW8SfZT",
    "outputId": "398c9065-13d5-4abe-a6ae-53b3a46492f5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "im feeling chills me to the bone tumbling been under down me mad and every showing sailing sailing life can would kind blue alive found think think dont dont lot never found am and friend girl me think think think do closed misunderstood never am again never chiquitita never andante never would had had had had stood slow lightly a christ kind realized dimension realized boomaboomerang love chills suffer and be cutting my lightly on ma ma ma ma ma ma ma ma ma ma ma ma ma ma ma ma ma ma ma ma ma ma ma ma ma ma ma ma\n"
     ]
    }
   ],
   "source": [
    "seed_text = \"im feeling chills\"\n",
    "next_words = 100\n",
    "  \n",
    "for _ in range(next_words):\n",
    "\ttoken_list = tokenizer.texts_to_sequences([seed_text])[0]\n",
    "\ttoken_list = pad_sequences([token_list], maxlen=max_sequence_len-1, padding='pre')\n",
    "\tpredicted = np.argmax(model.predict(token_list), axis=-1)\n",
    "\toutput_word = \"\"\n",
    "\tfor word, index in tokenizer.word_index.items():\n",
    "\t\tif index == predicted:\n",
    "\t\t\toutput_word = word\n",
    "\t\t\tbreak\n",
    "\tseed_text += \" \" + output_word\n",
    "print(seed_text)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "l10c03_nlp_constructing_text_generation_model.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
