<!DOCTYPE html>
<html>
  <head>
    <meta charset="UTF-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta
      name="description"
      content="Wrap-up TensorFlow Developer Certificate"
    />
    <meta name="keywards" content="TensorFlow Developer Certificate" />
    <meta name="author" content="Kritdikoon Woraitthinan" />
    <title>WRAP-UP: TensorFlow Developer Certificate</title>
    <link rel="stylesheet" href="/contents_resources/styles/mainStyles.css" />
    <link rel="icon" type="image/png" href="/images/J_favicon/favicon.ico" />

    <!-- Global site tag (gtag.js) - Google Analytics -->
    <script
      async
      src="https://www.googletagmanager.com/gtag/js?id=G-HDYFX99SK4"
    ></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag() {
        dataLayer.push(arguments);
      }
      gtag("js", new Date());

      gtag("config", "G-HDYFX99SK4");
    </script>
  </head>

  <body>
    <!-- NAVNIGATION BAR -->
    <p style="padding: 20px;"></p>

    <div class="navbar">
      <a class="main" href="https://jingwora.github.io">
        <img src="/images/J_favicon/favicon.ico" width="15" />
      </a>
      <a href="https://jingwora.github.io/contents.html">◀ CONTENTS</a>
    </div>

    <!-- CONTENT -->

    <h1>
      <img
        src="./TensorFlow Developer Certificate/img/tensorflow_logo.png"
        width="50"
      />
      WRAP-UP <br />
      TensorFlow Developer Certificate
    </h1>

    <!-- Table of Contents -->

    <h2>TOC</h2>
    <p><a href="#navh01">Overview▸</a></p>
    <p>
      <a href="#navh02"
        >Build and train neural network models using TensorFlow 2.x▸</a
      >
    </p>
    <p><a href="#navh03">Image classification▸</a></p>
    <p><a href="#navh04">Time series, sequences and predictions▸</a></p>
    <p><a href="#navh05">Natural language processing (NLP)▸</a></p>
    <p><a href="#navh06">Tensorflow Lite▸</a></p>
    <p><a href="#navh_resources">Resources▸</a></p>
    <hr />

    <!-- Overview -->

    <div id="navh01"></div>
    <div style="padding-top: 2em;"></div>
    <h2>Overview</h2>
    <p><a href="#TOP">[TOP]</a></p>

    <h2>TensorFlow Developer Certificate program Overview</h2>

    <p>
      The goal of this certificate is to provide everyone in the world the
      opportunity to showcase their expertise in ML in an increasingly AI-driven
      global job market. This certificate in TensorFlow development is intended
      as a foundational certificate for students, developers, and data
      scientists who want to demonstrate practical machine learning skills
      through the building and training of models using TensorFlow.
    </p>

    <p>
      The certificate program requires an understanding of building TensorFlow
      models using Computer Vision, Convolutional Neural Networks, Natural
      Language Processing, and real-world image data and strategies.
    </p>

    <hr />

    <h2>Artificial Intelligence</h2>
    <p>
      <b>Artificial Intelligence:</b> A field of computer science that aims to
      make computers achieve human-style intelligence. There are many approaches
      to reaching this goal, including machine learning and deep learning.
    </p>

    <ul>
      <li>
        <b>Machine Learning:</b>
        A set of related techniques in which computers are trained to perform a
        particular task rather than by explicitly programming them.
      </li>
      <li>
        <b>Neural Network:</b>
        A construct in Machine Learning inspired by the network of neurons
        (nerve cells) in the biological brain. Neural networks are a fundamental
        part of deep learning, and will be covered in this course.
      </li>
      <li>
        <b>Deep Learning:</b>
        A subfield of machine learning that uses multi-layered neural networks.
        Often, “machine learning” and “deep learning” are used interchangeably.
      </li>
    </ul>

    <a href="./TensorFlow Developer Certificate/img/ai-diagram.png">
      <img
        src="./TensorFlow Developer Certificate/img/ai-diagram.png"
        width="500px"
      />
    </a>

    <h1>Types of Machine Learning</h1>

    <h2>Supervised Learning</h2>
    <p>
      In supervised learning, the machine uses labeled training data. It is told
      the correct output and it compares its own output which informs the
      subsequent steps, adjusting itself along the way.
    </p>

    <p>The Supervised Learning mainly divided into two parts:</p>

    <h3>Regression</h3>
    <p>A machine learning model used to predict continuous value.</p>

    <p>
      <b>Regression algorithms</b> - predict one or more continuous numeric
      variables.
    </p>
    <p>
      <b>Association algorithms</b>
      - find correlations between different attributes in a dataset. The most
      common application of this kind of algorithm is for creating association
      rules, which can be used in a market basket analysis.
    </p>

    <h3>Classification</h3>
    <p>
      A machine learning model used for distinguishing among two or more output
      categories.
    </p>

    <h2>Unsupervised Learning</h2>
    <p>
      In unsupervised learning, the data isn't labeled. The machine must figure
      out the correct answer without being told and must therefore discover
      unknown patterns in the data.
    </p>
    <p>
      Some popular examples of unsupervised learning include GANs and
      Autoencoders.
    </p>
    <p>The Unsupervised Learning mainly divided into two parts:</p>

    <h3>Clustering</h3>
    <p>
      Clustering is a Machine Learning technique that involves the grouping of
      data points. Given a set of data points, we can use a clustering algorithm
      to classify each data point into a specific group.
    </p>

    <h3>Dimensionality Reduction</h3>
    <p>
      Machine Learning technique reduces the dimensionality of training data.
      The module analyzes input data and creates a reduced feature set that
      captures all the information contained in the dataset, but in a smaller
      number of features.
    </p>

    <h2>Semi-Supervised Learning</h2>
    <p>
      In Semi-Supervised Learning: Input data is a mixture of labeled and
      unlabeled examples.
    </p>

    <h2>Reinforcement Learning</h2>
    <p>
      Reinforcement Learning allows the machine the most freedom. It uses trial
      and error to discover the actions that yield the greatest rewards. AlphaGo
      is a famous example of RL.
    </p>

    <br />

    <h3>Machine learning vs Traditional programming</h3>
    <p>
      Traditional programming languages typically take data and rules as input
      and apply the rules to the data in order to come up with answers as
      output.
      <br />
      On the other hand, in machine learning paradigm the data and answers (or
      labels) go in as input and the learned rules (models) come out as output.
      Machine learning paradigm is uniquely valuable because it lets computer to
      learn new rules in complex and high dimensional space, a space harder to
      comprehend by humans.
    </p>

    <a href="./TensorFlow Developer Certificate/img/machine-learning-flow1.png">
      <img
        src="./TensorFlow Developer Certificate/img/machine-learning-flow1.png"
      />
    </a>
    <hr />
    <br />

    <!-- Build and train neural network models using TensorFlow 2.x -->

    <div id="navh02"></div>
    <div style="padding-top: 2em;"></div>
    <h1>Build and train neural network models using TensorFlow 2.x</h1>
    <p><a href="#TOP">[TOP]</a></p>

    <h3>The Training Process</h3>
    <p>
      The training process (happening in model.fit(...)) is really about tuning
      the internal variables of the networks to the best possible values, so
      that they can map the input to the output. This is achieved through an
      optimization process called Gradient Descent, which uses Numeric Analysis
      to find the best possible values to the internal variables of the model.
    </p>
    <p>
      <b>Gradient descent</b> iteratively adjusts parameters, nudging them in
      the correct direction a bit at a time until they reach the best values. In
      this case “best values” means that nudging them any more would make the
      model perform worse.
    </p>
    <p>
      The function that measures how good or bad the model is during each
      iteration is called the <b>“loss function”</b>, and the goal of each nudge
      is to “minimize the loss function.
    </p>

    <p>
      The training process starts with a forward pass, where the input data is
      fed to the neural network (see Fig.1). Then the model applies its internal
      math on the input and internal variables to predict an answer ("Model
      Predicts a Value" in Fig. 1). In our example, the input was the degrees in
      Celsius, and the model predicted the corresponding degrees in Fahrenheit.
    </p>

    <a href="./TensorFlow Developer Certificate/img/tensorflow-l2f2.png">
      <img
        src="./TensorFlow Developer Certificate/img/tensorflow-l2f2.png"
        width="700"
      />
    </a>

    <p>
      Once a value is predicted, the difference between that predicted value and
      the correct value is calculated. This difference is called the loss, and
      it's a measure of how well the model performed the mapping task. The value
      of the loss is calculated using a loss function, which we specified with
      the loss parameter when calling model.compile().
    </p>

    <p>
      After the loss is calculated, the internal variables (weights and biases)
      of all the layers of the neural network are adjusted, so as to minimize
      this loss — that is, to make the output value closer to the correct value
      (see Fig. 2).
    </p>

    <a href="./TensorFlow Developer Certificate/img/tensorflow-l2f3.png">
      <img
        src="./TensorFlow Developer Certificate/img/tensorflow-l2f3.png"
        width="700"
      />
    </a>

    <p>
      This optimization process is called Gradient Descent. The specific
      algorithm used to calculate the new value of each internal variable is
      specified by the optimizer parameter when calling model.compile(...). In
      this example we used the Adam optimizer.
    </p>

    <br />

    <h3>Key Machine Learning Terminology</h3>

    <p><b>Feature</b> - The input(s) to our model.</p>

    <p><b>Labels</b> - The output our model predicts.</p>

    <p>
      <b>Examples</b> - An input/output pair used for training. We break
      examples into two categories:
    </p>
    <ul>
      <li>
        Labeled example includes both feature(s) and the label.
        <br />
        <code>labeled examples: {features, label}: (x, y) </code>
      </li>
      <li>
        Unlabeled example contains features but not the label.
        <br />
        <code>unlabeled examples: {features, ?}: (x, ?) </code>
      </li>
    </ul>

    <p>
      <b>Model</b> - A model defines the relationship between features and
      label.
    </p>
    <p>
      <b>Training</b> - Training means creating or learning the model. That is,
      you show the model labeled examples and enable the model to gradually
      learn the relationships between features and label.
    </p>
    <p>
      <b>Inference</b> - Inference means applying the trained model to unlabeled
      examples. That is, you use the trained model to make useful predictions
      (y').
    </p>

    <br />

    <h3>Key Neural Network Terminology</h3>
    <p>
      <b>Layer</b> - A collection of nodes connected together within a neural
      network.
    </p>
    <p><b>Weights and biases</b> - The internal variables of model.</p>
    <p>
      <b>Loss</b> - The discrepancy between the desired output and the actual
      output
    </p>
    <p>
      <b>Gradient Descent</b> - An algorithm that changes the internal variables
      a bit at a time to gradually reduce the loss function.
    </p>
    <p>
      <b>Stochastic gradient descent (SGD)</b> - uses only a single example (a
      batch size of 1) per iteration. Given enough iterations, SGD works but is
      very noisy.
    </p>
    <p>
      <b>Mini-batch stochastic gradient descent (mini-batch SGD)</b> - is a
      compromise between full-batch iteration and SGD. A mini-batch is typically
      between 10 and 1,000 examples, chosen at random. Mini-batch SGD reduces
      the amount of noise in SGD but is still more efficient than full-batch.
    </p>
    <p>
      <b>Optimizer</b> - A specific implementation of the gradient descent
      algorithm.
    </p>
    <p>
      <b>Learning rate</b> - The “step size” for loss improvement during
      gradient descent.
    </p>
    <p>
      <b>Batch</b> - The set of examples used during training in a single
      iteration.
    </p>
    <p></p>
    <p><b>Epoch</b> - How many times this cycle should be run?</p>
    <p><b>Forward pass</b> - The computation of output values from input.</p>
    <p>
      <b>Backward pass (backpropagation)</b> - The calculation of internal
      variable adjustments according to the optimizer algorithm, starting from
      the output layer and working back through each layer to the input.
    </p>
    <p>
      <b>Verbose</b> - Argument controls how much output the method produces.
    </p>
    <p>
      <b>Flattening</b> - The process of converting a 2d image into 1d vector.
    </p>
    <p>
      <b>Softmax</b> - A function that provides probabilities for each possible
      output class.
    </p>

    <br />

    <h2>Colab: Notebook</h2>
    <p>
      <a
        href="./TensorFlow Developer Certificate/nb/l02c01_celsius_to_fahrenheit_wrap_up.html"
        target="_blank"
        >&#128212; Notebook: Celsius_to_fahrenheit_with_FC</a
      >
    </p>
    <a href="./TensorFlow Developer Certificate/img/celsius_to_fahrenheit.png">
      <img
        src="./TensorFlow Developer Certificate/img/celsius_to_fahrenheit.png"
        width="300"
      />
    </a>

    <p>
      <a
        href="./TensorFlow Developer Certificate/nb/l03c01_classifying_images_of_clothing_warp_up.html"
        target="_blank"
        >&#128212; Notebook: fashion_MNIST_with_FC</a
      >
    </p>
    <a href="./TensorFlow Developer Certificate/img/fashion_MNIST.png">
      <img
        src="./TensorFlow Developer Certificate/img/fashion_MNIST.png"
        width="500"
      />
    </a>

    <hr />
    <br />

    <!-- Image classification -->

    <div id="navh03"></div>
    <div style="padding-top: 2em;"></div>
    <h1>Image classification</h1>
    <p><a href="#TOP">[TOP]</a></p>

    <h2>Convolutional neural network</h2>
    <p>
      CNNs: Convolutional neural network. That is, a network which has at least
      one convolutional layer. A typical CNN also includes other types of
      layers, such as pooling layers and dense layers.
    </p>
    <p>
      <b>Convolution</b> - The process of applying a kernel (filter) to an
      image.
    </p>
    <p>
      <b>Kernel / filter</b> - A matrix which is smaller than the input, used to
      transform the input into chunks.
    </p>
    <p>
      <b>Padding</b> - Adding pixels of some value, usually 0, around the input
      image.
    </p>
    <p>
      <b>Pooling</b> - The process of reducing the size of an image through
      downsampling.There are several types of pooling layers. For example,
      average pooling converts many values into a single value by taking the
      average. However, maxpooling is the most common.
    </p>
    <p>
      <b>Maxpooling</b> - A pooling process in which many values are converted
      into a single value by taking the maximum value from among them.
    </p>
    <p>
      <b>Stride</b> - the number of pixels to slide the kernel (filter) across
      the image.
    </p>
    <p><b>Downsampling</b> - The act of reducing the size of an image.</p>
    <p>
      <b>Validation Set</b> - We use a validation set to check how the model is
      doing during the training phase. Validation sets can be used to perform
      Early Stopping to prevent overfitting and can also be used to help us
      compare different models and choose the best one.
    </p>

    <br />

    <h3>Color Images CNN</h3>

    <p>
      <b>Resizing</b> - When working with images of different sizes, you must
      resize all the images to the same size so that they can be fed into a CNN.
    </p>
    <p><b>Color Images</b> - Computers interpret color images as 3D arrays.</p>
    <p>
      <b>RGB Image</b> - Color image composed of 3 color channels: Red, Green,
      and Blue.
    </p>
    <p>
      <b>Convolutions</b> - When working with RGB images we convolve each color
      channel with its own convolutional filter. The result of each convolution
      is added up together with a bias value to get the convoluted output.
    </p>
    <p>
      <b>Max Pooling</b> - When working with RGB images we perform max pooling
      on each color channel using the same window size and stride. Max pooling
      on each color channel is performed in the same way as with grayscale
      images, i.e. by selecting the max value in each window.
    </p>

    <br />

    <h2>Overfitting</h2>
    <p>
      Overfitting refers to a model that fit the training data too well but
      cannot generalize well for new data.
    </p>

    <a href="./TensorFlow Developer Certificate/img/overfitting.png">
      <img
        src="./TensorFlow Developer Certificate/img/overfitting.png"
        width="500"
      />
    </a>

    <h3>Techniques to Prevent Overfitting</h3>
    <p>
      <b>Early Stopping</b> - In this method, we track the loss on the
      validation set during the training phase and use it to determine when to
      stop training such that the model is accurate but not overfitting.
    </p>

    <p>
      <b>Image Augmentation</b> - Artificially boosting the number of images in
      our training set by applying random image transformations to the existing
      images in the training set.
    </p>
    <p>
      <b>Dropout</b> - Removing a random selection of a fixed number of neurons
      in a neural network during training.
    </p>
    <br />

    <h2>Colab: Notebook CNN</h2>
    <p>
      <a
        href="./TensorFlow Developer Certificate/nb/l04c01_image_classification_with_cnns_wrap_up.html"
        target="_blank"
        >&#128212; Notebook: fashion_MNIST_with_cnns</a
      >
    </p>

    <p>
      <a
        href="./TensorFlow Developer Certificate/nb/l05c01_dogs_vs_cats_without_augmentation_wrap_up.html"
        target="_blank"
        >&#128212; Notebook: dogs_vs_cats_without_augmentation</a
      >
    </p>

    <p>
      <a
        href="./TensorFlow Developer Certificate/nb/l05c02_dogs_vs_cats_with_augmentation_wrap_up.html"
        target="_blank"
        >&#128212; Notebook: dogs_vs_cats_with_augmentation</a
      >
    </p>

    <p>
      <a
        href="./TensorFlow Developer Certificate/nb/l05c03_exercise_flowers_with_data_augmentation_wrap_up.html"
        target="_blank"
        >&#128212; Notebook: flowers_with_data_augmentation</a
      >
    </p>

    <hr />
    <br />

    <h2>Transfer Learning</h2>

    <p>
      <b>Transfer Learning</b> - A technique that reuses a model that was
      created by machine learning experts and that has already been trained on a
      large dataset. When performing transfer learning we must always change the
      last layer of the pre-trained model so that it has the same number of
      classes that we have in the dataset we are working with.
    </p>

    <a href="./TensorFlow Developer Certificate/img/transfer-learning.png">
      <img
        src="./TensorFlow Developer Certificate/img/transfer-learning.png"
        width="500"
      />
    </a>

    <p>
      <b>Freezing Parameters</b> - Setting the variables of a pre-trained model
      to non-trainable. By freezing the parameters, we will ensure that only the
      variables of the last classification layer get trained, while the
      variables from the other layers of the pre-trained model are kept the
      same.
    </p>
    <p>
      <b>MobileNet</b> - A state-of-the-art convolutional neural network
      developed by Google that uses a very efficient neural network architecture
      that minimizes the amount of memory and computational resources needed,
      while maintaining a high level of accuracy. MobileNet is ideal for mobile
      devices that have limited memory and computational resources.
    </p>

    <h2>Colab: Notebook Transfer Learning</h2>

    <p>
      <a
        href="./TensorFlow Developer Certificate/nb/l06c01_Image_classification_TF_Dog_vs_Cat_with_transfer_learning.html"
        target="_blank"
        >&#128212; Notebook: Dog vs Cat and Transfer Learning</a
      >
    </p>

    <p>
      <a
        href="./TensorFlow Developer Certificate/nb/l06c03_Image_classification_TF_Flowers_with_transfer_learning.html"
        target="_blank"
        >&#128212; Notebook: Flower and Transfer Learning</a
      >
    </p>

    <hr />
    <br />

    <h2>Saving and Loading Models</h2>

    <a href="./TensorFlow Developer Certificate/img/saving-loading-models.png">
      <img
        src="./TensorFlow Developer Certificate/img/saving-loading-models.png"
        width="600"
      />
    </a>
    <p>
      We can save trained model as HDF5 file, which is the format used by Keras.
      Then we can load the model. The model can be retrained and save.
    </p>
    <p>
      This file includes: <br />
      - The model's architecture<br />
      - The model's weight values (which were learned during training) <br />
      - The model's training config (what you passed to `compile`), if any
      <br />
      - The optimizer and its state, if any (this enables you to restart
      training where you left off)
    </p>

    <h3>Export as SavedModel</h3>
    <p>
      You can also export a whole model to the TensorFlow SavedModel format.
      SavedModel is a standalone serialization format for Tensorflow objects,
      supported by TensorFlow serving as well as TensorFlow implementations
      other than Python. A SavedModel contains a complete TensorFlow program,
      including weights and computation. It does not require the original model
      building code to run, which makes it useful for sharing or deploying (with
      TFLite, TensorFlow.js, TensorFlow Serving, or TFHub).
    </p>

    <p>
      The SavedModel files that were created contain: * A TensorFlow checkpoint
      containing the model weights. * A SavedModel proto containing the
      underlying Tensorflow graph. Separate graphs are saved for prediction
      (serving), train, and evaluation. If the model wasn't compiled before,
      then only the inference graph gets exported. * The model's architecture
      config, if available.
    </p>
    <p>
      Let's save our original `model` as a TensorFlow SavedModel. To do this we
      will use the `tf.saved_model.save()` function. This functions takes in the
      model we want to save and the path to the folder where we want to save our
      model. This function will create a folder where you will find an `assets`
      folder, a `variables` folder, and the `saved_model.pb` file.
    </p>

    <h2>Colab: Notebook SavedModel</h2>

    <p>
      <a
        href="./TensorFlow Developer Certificate/nb/l07c01_saving_and_loading_models.html"
        target="_blank"
        >&#128212; Notebook: Dog vs Cat and Saving and Loading Model</a
      >
    </p>

    <hr />
    <br />

    <h2>Neural Network Structures</h2>

    <h3>Dense and Fully Connected (FC)</h3>
    <p>
      Each node in one layer is connected to each node in the previous layer.
    </p>
    <hr />
    <br />

    <h2>Activations</h2>

    <h3>The Rectified Linear Unit (ReLU)</h3>
    <p>
      ReLU function gives an output of 0 if the input is negative or zero, and
      if input is positive, then the output will be equal to the input.
    </p>
    <p>ReLU gives the network the ability to solve nonlinear problems.</p>
    <hr />
    <br />

    <h2>Training and Test Sets: Splitting Data</h2>

    <p>Make sure that your test set meets the following two conditions:</p>
    <ul>
      <li>Is large enough to yield statistically meaningful results.</li>
      <li>
        Is representative of the data set as a whole. In other words, don't pick
        a test set with different characteristics than the training set.
      </li>
    </ul>

    <hr />

    <h2>Model Evaluation</h2>
    <h3>Model Evaluation: Regression</h3>

    <p>
      <b>Mean square error (MSE)</b>
      <br />
      The average squared loss per example over the whole dataset.
    </p>

    <h3>Model Evaluation: Classification</h3>

    <hr />

    <!-- Time series, sequences and predictions -->

    <div id="navh04"></div>
    <div style="padding-top: 2em;"></div>
    <h1>Time series, sequences and predictions</h1>
    <p><a href="#TOP">[TOP]</a></p>

    <p>
      Time series is ordered sequence of value spread over time.
      Time series data is available in various place like stock price, 
      weather forecast, demand trend, audio, GPS location log, brain wave etc.
    </p>
    <p>
      <b>Univariate time series</b> is time series data with single value at each time-step.
      <br>
      <b>Multivariate time series</b> is time series with multiple value at each time-step.
    </p>

    <p>
      <b>Forecasting</b> – process of predicting the future based on past and
       present data. Use cases: Demand forecast, Weather forecast, …
       <br>
      <b>Anomaly Detection</b> – process of identifying unusual data points in dataset. 
       Use cases: Spam detection, fraud detection, …

    </p>
    <hr>

    <h3>Time series patterns</h3>
    <p>
      <b>Trend</b> - A trend exists when there is a long-term increase or decrease in the data.
      <br>
      <b>Seasonality</b> - A seasonal pattern occurs when a time series is affected by seasonal factors such as the day of week.
      <br>
      <b>Cyclic</b> - A cycle occurs when the data exhibit rises and falls that are not of a fixed frequency.
      <br>
      <b>Noise</b> - Noise simply refers to random fluctuations of data.

    </p>
    <hr><br>



    <h2>Forecasting Approaches</h2>
    <h3>Naïve Forecasting</h3>
    <p>
      Last period's actuals are used as this period's forecast. 
      It is used as a based model for comparison.
    </p>

    <h3>Moving Average Forecasting</h3>
    <p>
      Average value of moving window data used to smooth data.
      <br>
      Differencing – calculate different of each data point to 
      previous period for example -1 year to eliminate trend and seasonality. 
      (series(t) – series(t-365)
    </p>

    <h3>Linear Regression Forecasting</h3>
    <p>
      Linear relationship model with 1 dense layer.
    </p>

    <h3>RNN (Recurrent Neural Network)</h3>
    <p>
      RNN is a neural network which contains recurrent layers, that can sequentially 
      process a sequence of inputs. RNN contain memory cell which is used repeatedly 
      to compute the outputs. RNN expects 3D inputs: batch size, number of time-steps 
      and series dimension.
      <br><br>
      <img
      src="./TensorFlow Developer Certificate/img/forecasting_RNN.png"
      width="500"
      />

      <br>
      RNN > RNN > Dense
      <br>
      Sequent to Vector RNNs
      <br>
      Sequent to Sequent RNNs
      <br>
      <b>Stateless RNNs</b> – NN resets state to zero at each iteration. Stateless 
      RNNs is simple to use but it cannot learn patterns longer than the length of the window.
      <br><br>
      <img
      src="./TensorFlow Developer Certificate/img/forecasting_stateless_RNN.png"
      width="500"
      />
      <br>
      <b>Stateful RNNs</b> – NN which state is preserved for next training batch and 
      reset state to zero at the end of each epoch. It can learn from previous 
      state yet, it required more data preparation and take more time. 
      (used less compared to Stateless RNNs)
      <br><br>
      <img
      src="./TensorFlow Developer Certificate/img/forecasting_stateful_RNN.png"
      width="500"
      />
    </p>

    <h3>LSTM (long short-term memory)</h3>
    <p>
      LSTM has short term and long time memory cell. LTSM call can detect 
      patterns of over 100 time steps.
    </p>
    <img
    src="./TensorFlow Developer Certificate/img/forecasting_LSTM.png"
    width="500"
    />

    <h3>CNN (Convolutional Neural Network)</h3>
    <p>
      it is preferable to stack multiple convolutional layers with small kernel 
      which increase performance and accuracy. Hyperparameter: Padding, Stride, 
      No. of filter, Kernels
    </p>

    <img
    src="./TensorFlow Developer Certificate/img/forecasting_1D_CNN.png"
    width="500"
    />

    <h3>WaveNet</h3>
    <p>
      WaveNet is CNN model that has receptive field at different dilation rate 
      to learn different time frame.
    </p>
    <img
    src="./TensorFlow Developer Certificate/img/forecasting_WaveNet.png"
    width="500"
    />
    <hr>
    <br>

    <h3>Forecasting performance evaluation</h3>
    <p>
      We use train and validation data to evaluate model and use full data including 
      test period for production deployment. 
      <br>
      <b>Fix partitioning</b> - Split data into - Train, Validation, Test period
      <br>
      <b>Roll-forward partitioning</b> – Create series window of Train and Validation. 
      It required much more training time but it more mimic production condition.
    </p>

    <h3>Evaluation Metrics</h3>
    <table class="t1">
      <tr>
        <th>Metrics</th>
        <th>Explaination</th>
      </tr>
      <tr>
        <td>errors</td>
        <td>forecast – actual</td>
      </tr>
      <tr>
        <td>mse <br> (mean square error)</td>
        <td>np.square(errors).mean()</td>
      </tr>
      <tr>
        <td>rmse <br> (root mean square error) </td>
        <td>pow(mse, 2)</td>
      </tr>
      <tr>
        <td>mae <br> (mean absolute error) </td>
        <td>np.abs(errors).mean()</td>
      </tr>
      <tr>
        <td>mape  <br> (mean absolute percentage error) </td>
        <td>np.abs(errors/x_valid).mean()</td>
      </tr>
    </table>
    <br />

  
    <h2>Colab: Notebook SavedModel</h2>

    <p>
      <a
        href="./TensorFlow Developer Certificate/nb/l08c01_time_series_patterns.html"
        target="_blank"
        >&#128212; Notebook: Time series patterns</a
      >
    </p>

    <p>
      <a
        href="./TensorFlow Developer Certificate/nb/l08c03_forecasting.html"
        target="_blank"
        >&#128212; Notebook: Forecasting</a
      >
    </p>

    <p>
      <a
        href="./TensorFlow Developer Certificate/nb/l08c05_forecasting_with_machine_learning.html"
        target="_blank"
        >&#128212; Notebook: Forecasing with Machine Learning</a
      >
    </p>

    <p>
      <a
        href="./TensorFlow Developer Certificate/nb/l08c06_forecasting_with_rnn.html"
        target="_blank"
        >&#128212; Notebook: Forecasting with RNN</a
      >
    </p>

    <p>
      <a
        href="./TensorFlow Developer Certificate/nb/l08c07_forecasting_with_stateful_rnn.html"
        target="_blank"
        >&#128212; Notebook: Forecasting wiht Stateful RNN</a
      >
    </p>

    <p>
      <a
        href="./TensorFlow Developer Certificate/nb/l08c08_forecasting_with_lstm.html"
        target="_blank"
        >&#128212; Notebook: Forecasting wiht LSTM</a
      >
    </p>

    <p>
      <a
        href="./TensorFlow Developer Certificate/nb/l08c09_forecasting_with_cnn.html"
        target="_blank"
        >&#128212; Notebook: Forecasting wiht CNN</a
      >
    </p>

    <br />
    <hr />

    <!-- Natural language processing (NLP) -->

    <div id="navh05"></div>
    <div style="padding-top: 2em;"></div>
    <h1>Natural language processing (NLP)</h1>
    <p><a href="#TOP">[TOP]</a></p>

    <p>
      Natural Language Processing, or NLP for short, focuses on analyzing text
      and speech data. This can range from simple recognition (what words are in
      the given text/speech), to sentiment analysis (was a review positive or
      negative), and all the way to areas like text generation (creating novel
      song lyrics from scratch).
    </p>

    <p>
      NLP got its start mostly on machine translation, where users often had to
      create strict, manual rules to go from one language to another. It has
      since morphed to be more machine learning-based, reliant on much larger
      datasets than the early methods were.
    </p>

    <img
      src="./TensorFlow Developer Certificate/img/sentiment-example.png"
      width="500"
    />
    <br />

    <h3>Contents:</h3>
    <p>
      <b>Tokenization</b><br />

      - Tokenizing<br />
      - Padding sequences<br />
      - OOV (Out of vocabulary words)<br />
      - Use real datasets<br />
    </p>

    <p>
      <b>Embeddings</b><br />
      - Transform into Enbeddings<br />
      - Develop a Sentiment Model<br />
      - Visualize Embedding Space<br />
      - Tweak Hyperparameters<br />
      - Diagnose Sequence Issues
    </p>

    <p>
      <b>RNN</b><br />
      - Sequence Context<br />
      - RNN Architecture<br />
      - Usage of LSTMs<br />
      - LSTM vs Convolution vs GRU<br />
    </p>

    <p>
      <b>Text Generation</b><br />
      - Text Generation Basics<br />
      - LSTMs for Text Generation<br />
      - Optimizing Text Generation
    </p>

    <br />
    <hr />

    <h2>Text Preprocessing</h2>

    <h3>Tokenizing Text</h3>

    <p>
      Neural networks utilize numbers as their inputs, so we need to convert our
      input text into numbers. Tokenization is the process of assigning numbers
      to our inputs, but there is more than one way to do this - should each
      letter have its own numerical token, each word, phrase, etc.
    </p>

    <p>
      Tokenizing based on letters with our current neural networks doesn’t
      always work so well - anagrams, for instance, may be made up of the same
      letters but have vastly different meanings. So, in our case, we’ll start
      by tokenizing each individual word.
    </p>

    <p>
      <b>Tokenizer</b>
      <br />
      With TensorFlow, this is done easily through use of a Tokenizer, found
      within tf.keras.preprocessing.text. If you wanted only the first 10 most
      common words, you could initialize it like so:
      <br />
      <code>tokenizer = Tokenizer(num_words=10)</code>
    </p>

    <p>
      <b>Fit on Texts </b>
      <br />
      Then, to fit the tokenizer to your inputs (in the below case a list of
      strings called sentences), you use .fit_on_texts():
      <br />
      <code>tokenizer.fit_on_texts(sentences) </code>
    </p>

    <p>
      <b>Text to Sequences </b>
      <br />
      From there, you can use the tokenizer to convert sentences into tokenized
      sequences:
      <br />
      <code>tokenizer.texts_to_sequences(sentences) </code>
    </p>

    <p>
      <b>Out of Vocabulary Words (OOV) </b>
      <br />
      However, new sentences may have new words that the tokenizer was not fit
      on. By default, the tokenizer will just ignore these words and not include
      them in the tokenized sequences. However, you can also add an “out of
      vocabulary”, or OOV, token to represent these words. This has to be
      specified when originally creating the Tokenizer object.
      <br />
      <code>tokenizer = Tokenizer(num_words=20, oov_token=’OOV’) </code>
    </p>

    <p>
      <b>Viewing the Word Index </b>
      <br />
      Lastly, if you want to see how the tokenizer has mapped numbers to words,
      use the tokenizer.word_index property to see this mapping.
      <br />
      <code>tokenizer = Tokenizer(num_words=20, oov_token=’OOV’) </code>
    </p>

    <h3>Colab: Notebook</h3>

    <p>
      <a
        href="./TensorFlow Developer Certificate/nb/l09c01_nlp_turn_words_into_tokens.html"
        target="_blank"
        >&#128212; Notebook: NLP Turn words into Tokens</a
      >
    </p>

    <h3>padding and truncating</h3>

    <p>
      Even after converting sentences to numerical values, there’s still an
      issue of providing equal length inputs to our neural networks - not every
      sentence will be the same length! There’s two main ways you can process
      the input sentences to achieve this - padding the shorter sentences with
      zeroes, and truncating some of the longer sequences to be shorter. In
      fact, you’ll likely use some combination of these.
    </p>

    <hr />
    <br />

    <p>
      <b>pad_sequences </b>
      <br />
      With TensorFlow, the pad_sequences function from
      tf.keras.preprocessing.sequence can be used for both of these tasks. Given
      a list of sequences, you can specify a maxlen (where any sequences longer
      than that will be cut shorter), as well as whether to pad and truncate
      from either the beginning or ending, depending on pre or post settings for
      the padding and truncating arguments. By default, padding and truncation
      will happen from the beginning of the sequence, so set these to post if
      you want it to occur at the end of the sequence. If you wanted to pad and
      truncate from the beginning, you could use the following:
      <br />
      <code>padded = pad_sequences(sequences, maxlen=10) </code>
    </p>

    <h3>Colab: Notebook</h3>

    <p>
      <a
        href="./TensorFlow Developer Certificate/nb/l09c02_nlp_padding.html"
        target="_blank"
        >&#128212; Notebook: NLP Padding</a
      >
    </p>

    <p>
      <a
        href="./TensorFlow Developer Certificate/nb/l09c03_nlp_prepare_larger_text_corpus.html"
        target="_blank"
        >&#128212; Notebook: NLP prepare larger text corpus</a
      >
    </p>

    <hr />
    <br />

    <h3>Word Embeddings</h3>

    <p>
      Embeddings are clusters of vectors in multi-dimensional space, where each
      vector represents a given word in those dimensions. While it’s difficult
      for us humans to think in many dimensions, luckily the TensorFlow
      Projector makes it fairly easy for us to view these clusters in a 3D
      projection (later Colabs will generate the necessary files for use with
      the projection tool). This can be very useful for sentiment analysis
      models, where you’d expect to see clusters around either more positive or
      more negative sentiment associated with each word.
    </p>
    <img
      src="./TensorFlow Developer Certificate/img/sphereized-viz.png"
      width="500"
    />

    <p>
      To create our embeddings, we’ll first use an embeddings layer, called
      tf.keras.layers.Embedding. This takes three arguments: the size of the
      tokenized vocabulary, the number of embedding dimensions to use, as well
      as the input length (from when you standardized sequence length with
      padding and truncation).
    </p>
    <p>
      The output of this layer needs to be reshaped to work with any
      fully-connected layers. You can do this with a pure Flatten layer, or use
      GlobalAveragePooling1D for a little additional computation that sometimes
      creates better results. In our case, we’re only looking at positive vs.
      negative sentiment, so only a single output node is needed (0 for
      negative, 1 for positive). You’ll be able to use a binary cross entropy
      loss function since the result is only binary classification.
    </p>
    <br />

    <b>A Note on Embedding Networks </b>
    <p>
      They suggest that the final network does not use a sigmoid activation
      layer when working with embeddings, especially when using just the two
      classes like we are for sentiment analysis:
      <br />
      <code>tf.keras.layers.Dense(1) </code>
    </p>
    <p>
      <br />
      Additionally, they suggest instead of using the string
      ”binary_crossentropy” as the loss function, you use
      <br />
      <code>tf.keras.losses.BinaryCrossentropy </code>
      <code>(from_logits=True)</code>
    </p>
    <br />

    <h3>Visualizing Embeddings</h3>
    <p>
      We’ve given you the code to create the files for input into the projector.
      This will download two files: 1) the vectors, and 2) the metadata. The
      projector will already come with a pre-loaded visualization, so you’ll
      need to use the “Load” button on the left and upload each of the two
      files.
    </p>
    <p>
      In some cases, there may be a small difference in the number of tensors
      present in the vector file and the metadata file (usually with a message
      appearing after uploading the metadata); if this appears, wait for a few
      seconds for the error message to disappear, and then click outside the
      window. Typically, the visualization will still load just fine. Make sure
      to click the checkbox for “Sphereize data”, which will better show whether
      there is separation between positive and negative sentiment (or not).
    </p>

    <p>
      <a href="http://projector.tensorflow.org/" target="_blank"
        >&#127760; Tensorflow Projector</a
      >
    </p>
    <br />

    <h3>Colab: Notebook</h3>

    <p>
      <a
        href="./TensorFlow Developer Certificate/nb/l09c04_nlp_embeddings_and_sentiment.html"
        target="_blank"
        >&#128212; Notebook: NLP embedding</a
      >
    </p>

    <hr />
    <br />

    <h3>Tweaking the Model</h3>

    <p>
      There are a number of ways in which you might improve the sentiment
      analysis model we’ve built already:
    </p>

    <p>
      Data and preprocessing-based approaches<br />
      - More data<br />
      - Adjusting vocabulary size (make sure to consider the overall size of the
      corpus!)<br />
      - Adjusting sequence length (more or less padding or truncation)<br />
      - Whether to pad or truncate pre or post (usually less of an effect than
      the others)
    </p>

    <p>
      Model-based approaches<br />
      - Adjust the number of embedding dimensions<br />
      - Changing use of Flatten vs. GlobalAveragePooling1D<br />
      - Considering other layers like Dropout<br />
      - Adjusting the number of nodes in intermediate fully-connected layers<br />
      These are just some of the potential things you might tweak to better
      predict sentiment from text.
    </p>

    <hr />
    <br />
    <h2>Subword</h2>
    <p>
      Subwords are another approach, where individual words are broken up into
      the more commonly appearing pieces of themselves. This helps avoid marking
      very rare words as OOV when you use only the most common words in a
      corpus.
    </p>

    <img
      src="./TensorFlow Developer Certificate/img/subwords.png"
      width="300"
    />

    <p>
      recurrent neural networks will help understanding the full context of the
      sequence of words in an input.
    </p>

    <h3>Subword Datasets</h3>
    <p>
      There are a number of already created subwords datasets available online.
      If you check out the IMDB dataset on TFDS, for instance, by scrolling down
      you can see datasets with both 8,000 subwords as well as 32,000 subwords
      in a corpus (along with regular full-word datasets).
    </p>

    <p>
      TensorFlow’s SubwordTextEncoder and its build_from_corpus function is used
      to create subwords from the reviews dataset we used previously.
    </p>

    <h3>Colab: Notebook</h3>

    <p>
      <a
        href="./TensorFlow Developer Certificate/nb/l09c06_nlp_subwords.html"
        target="_blank"
        >&#128212; Notebook: NLP Subword</a
      >
    </p>

    <h3>Datasets</h3>

    <p>
      <a href="https://github.com/niderhoff/nlp-datasets" target="_blank"
        >&#127760; A popular Github repo for NLP datasets
      </a>
    </p>

    <p>
      <a href="https://www.kaggle.com/datasets" target="_blank"
        >&#127760; Kaggle datasets
      </a>
    </p>

    <p>
      <a href="https://datasetsearch.research.google.com/" target="_blank"
        >&#127760; Google's public dataset search
      </a>
    </p>

    <hr />
    <br />

    <h2>Recurrent Neural Networks (RNNs)</h2>
    <p>
      Recurrent Neural Networks (RNNs) still take in some input x and output
      some y, but they also feed some of the output of the network back into
      itself. This may be done over and over, so that with text input, the
      network has some memory of words that came much earlier in a sequence.
    </p>

    <img
      src="./TensorFlow Developer Certificate/img/rnn-basics.png"
      width="300"
    />

    <p>
      Simple RNNs are not always enough when working with text data. Longer
      sequences, such as a paragraph, often are difficult to handle, as the
      simple RNN structure loses information about previous inputs fairly
      quickly.
    </p>

    <h2>LTSTMs (Long Short-Term Memory models)</h2>
    <p>
      Long Short-Term Memory models, or LSTMs, help resolve this by keeping a
      “cell state” across time. These include a “forget gate”, where the cell
      can choose whether to keep or forget certain words to carry forward in the
      sequence. Another interesting aspect of LSTMs is that they can be
      bidirectional, meaning that information can be passed both forward (later
      in the text sequence) and backward (earlier in the text sequence).
    </p>

    <img
      src="./TensorFlow Developer Certificate/img/lstm_forget.png"
      width="500"
    />

    <h3>LSTMs in Code</h3>
    <p>
      The code for an LSTM layer itself is just the LSTM layer from
      tf.keras.layers, with the number of LSTM cells to use. However, this is
      typically wrapped within a Bidirectional layer to make use of passing
      information both forward and backward in the network, as we noted on the
      previous page.
    </p>

    <code>tf.keras.layers.Bidirectional </code>
    <code>(tf.keras.layers.LSTM(64))</code>

    <p>
      One thing to note when using a Bidirectional layer is when you look at the
      model summary, if you put in 64 LSTM nodes, you will actually see a layer
      shape with 128 nodes (64x2).
    </p>

    <p>
      <b>No Need to Flatten </b><br />
      Unlike our more vanilla neural networks in the last lesson, you no longer
      need to use Flatten or GlobalAveragePooling1D after the LSTM layer - the
      LSTM can take the output of an Embedding layer and directly hook up to a
      fully-connected Dense layer with its own output.
    </p>

    <p>
      <b>Doubling Up </b><br />
      You can also feed an LSTM layer into another LSTM layer. To do so, on top
      of just stacking them in order when you create the model, you also need to
      set return_sequences to True for the earlier LSTM layer - otherwise, as
      noted above, the output will be ready for fully-connected layers and not
      be in the sequence format the LSTM layer expects.
    </p>

    <textarea class="ta2">
    # Two bidirectional LSTM layers with 64 nodes each
    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64),
    return_sequences=True)
    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64))
    </textarea>

    <h3>Colab: Notebook</h3>

    <p>
      <a
        href="./TensorFlow Developer Certificate/nb/l10c01_nlp_lstms_with_reviews_subwords_dataset.html"
        target="_blank"
        >&#128212; Notebook: LSTMs with Subword</a
      >
    </p>

    <hr />
    <br />

    <h2>LSTMs vs. Convolutions vs. GRUs</h2>

    <p>
      <b>Convolutional Layers for Text </b><br />
      Just like you did with images, you can also use convolutional layers on
      text, where the convolution occurs across a sequence of words instead of
      across an image. To use a convolutional layer on text inputs, you can
      place a Conv1D layer directly after the Embedding layer:
      <code>tf.keras.layers.Conv1D </code><br />
      <code>(128, 5, activation=’relu’)</code>
    </p>

    <p>
      Note that you will need to use Flatten or GlobalAveragePooling1D on the
      output of this layer to connect to any fully-connected layers from there.
    </p>

    <p>
      <b>GRUs(Gated Recurrent Units) </b><br />
      Gated Recurrent Units, or GRUs, have “update” and “reset” gates. These
      gates decide what to keep and what to throw away. They do not have a “cell
      state” like LSTMs do. The code for these is very similar to an LSTM, where
      the GRU layer is wrapped in a Bidirectional layer.
      <code>tf.keras.layers.Bidirectional </code><br />
      <code>(tf.keras.layers.GRU(32))</code>
    </p>

    <p>
      An important item to note here is these performance differences (with
      perhaps the exception of training duration) will vary depending on the
      dataset and other changes to the model - you shouldn’t always assume one
      type of model will work better than another.
    </p>

    <h3>In short</h3>

    <p>
      <b>CNN</b><br />
      Utilizes “filters” that can slide over multiple words at a time and
      extract features from those sequences of words. Can be used for purposes
      other than a recurrent neural network.
    </p>

    <p>
      <b>GRU</b><br />
      Utilizes “update” and “reset” gates, where the “update” gate determines
      updates to the existing stored knowledge, and the reset gate determines
      how much to forget in the existing stored knowledge.
    </p>

    <p>
      <b>LSTM </b><br />
      Utilizes “forget” and “learn” gates that feed to “remember” and “use”
      gates, where remembering is for further storage for the next input, and
      using is for generating the current output.
    </p>

    <h3>Colab: Notebook</h3>

    <p>
      <a
        href="./TensorFlow Developer Certificate/nb/l10c02_nlp_multiple_models_for_predicting_sentiment.html"
        target="_blank"
        >&#128212; Notebook: NLP with multiple models comparison</a
      >
    </p>

    <h3>Document</h3>
    <p>
      <a
        href="https://www.tensorflow.org/api_docs/python/tf/keras/layers/GRU"
        target="_blank"
        >&#127760; Documentation for GRU layers in TensorFlow
      </a>
    </p>
    <p>
      <a
        href="https://www.tensorflow.org/api_docs/python/tf/keras/layers/LSTM"
        target="_blank"
        >&#127760; Documentation for LSTM layers in TensorFlow
      </a>
    </p>
    <p>
      <a
        href="https://www.tensorflow.org/api_docs/python/tf/keras/layers/Conv1D"
        target="_blank"
        >&#127760; Documentation for 1D Convolutional layers in TensorFlow
      </a>
    </p>

    <hr />
    <br />

    <h2>Text Generation</h2>

    <p>
      Text generation can be done through simply predicting the next most likely
      word, given an input sequence. This can be done over and over by feeding
      the original input sequence, plus the newly predicted end word, as the
      next input sequence to the model. As such, the full output generated from
      a very short original input can effectively go on however long you want it
      to be.
    </p>

    <p>
      The only real change to the network here is the output layer will now be
      equivalent to a node per each possible new word to generate - so, if you
      have 1,000 possible words in your corpus, you’d have an output array of
      length 1,000. You’ll also need to change the loss function from binary
      cross-entropy to categorical cross entropy - before, we had only a 0 or 1
      as output, now there are potentially thousands of output “classes” (each
      possible word).
    </p>

    <img
      src="./TensorFlow Developer Certificate/img/text-prediction-example.png"
      width="400"
    />

    <p>
      There are hardly any differences in the model itself, other than changing
      the number of nodes in the output layer and changing the loss function.
      The more obvious changes come in working with the input and output data.
      The input data takes chunks of sequences and just splits off the final
      word as its label. So, if we had the sentence “I went to the beach with my
      dog”, and we had a max input length of five words, we’d get:
    </p>
    <p>
      Input: I went to the beach <br />
      Label: with
    </p>
    <p>
      Now, that’s not the only sequence that will come from the sentence! We
      would also get:
    </p>
    <p>
      Input: went to the beach with
      <br />
      Label: my
    </p>
    <p>And</p>
    <p>
      Input: to the beach with my
      <br />
      Label: dog
    </p>

    <p>
      That’s how the N-Grams used in the pre-processing work - a single input
      sequence might actually become a series of sequences and labels. With the
      output of the network, I’ll let you mostly investigate that code in the
      Colab on the next page, but the important thing there is that you can keep
      looping and creating more text by just appending the next word onto the
      previous input sequence.
    </p>

    <h3>Colab: Notebook</h3>

    <p>
      <a
        href="./TensorFlow Developer Certificate/nb/l10c03_nlp_constructing_text_generation_model.html"
        target="_blank"
        >&#128212; Notebook: NLP CNN song generation</a
      >
    </p>
    <br />

    <h2>Optimizing the Text Generation Model</h2>

    <p>
      There are a number of ways you might improve your text generation
      models:<br />

      Using more data?<br />
      You’ll need to consider memory and output size constraints<br />
      - Also consider using only top-k most common words<br />
      - Know your data - songs have many more words than a Tweet<br />
      Keep tuning your model<br />
      - Add/subtract from layer sizes or embedding dimensions<br />
      Use np.random.choice with the probabilities for more variance in predicted
      outputs
    </p>

    <h3>Colab: Notebook</h3>

    <p>
      <a
        href="./TensorFlow Developer Certificate/nb/l10c04_nlp_optimizing_the_text_generation_model.html"
        target="_blank"
        >&#128212; Notebook: NLP optimizing song generation</a
      >
    </p>
    <br />

    <hr />
    <br />

    <div id="navh06"></div>
    <div style="padding-top: 2em;"></div>
    <h2>Tensorflow Lite</h2>
    <p><a href="#TOP">[TOP]</a></p>

    <hr />
    <br />

    <div id="navh_resources"></div>
    <div style="padding-top: 2em;"></div>
    <h2>Resources</h2>
    <p><a href="#TOP">[TOP]</a></p>

    <p>
      <a href="https://developers.google.com/certification" target="_blank"
        >&#127891; Google Developers Certification
      </a>
    </p>

    <p>
      <a href="https://www.tensorflow.org/certificate" target="_blank"
        >&#127891; TensorFlow Developer Certificate
      </a>
    </p>

    <p>
      <a
        href="https://www.tensorflow.org/site-assets/downloads/marketing/cert/TF_Certificate_Candidate_Handbook.pdf"
        target="_blank"
        >&#128229; Candidate Handbook
      </a>
    </p>

    <p>
      <a
        href="https://www.coursera.org/specializations/tensorflow-in-practice#howItWorks"
        target="_blank"
        >&#127891; Course: Deeplearning.ai TensorFlow in Practice Specialization
      </a>
    </p>

    <p>
      <a
        href="https://www.coursera.org/specializations/tensorflow-in-practice#howItWorks"
        target="_blank"
        >&#127891; Course: Deeplearning.ai TensorFlow in Practice Specialization
      </a>
    </p>

    <p>
      <a
        href="https://developers.google.com/machine-learning/crash-course/ml-intro"
        target="_blank"
        >&#127891; Course: Machine Learning Crash Course
      </a>
    </p>

    <p>
      <a
        href="http://introtodeeplearning.com/"
        target="_blank"
        >&#127891; Course: MIT Introdduction to Deep Learning
      </a>
    </p>

    <p>
      <a
        href="https://www.youtube.com/playlist?list=PLQY2H8rRoyvwLbzbnKJ59NkZvQAW9wLbx"
        target="_blank"
        >&#127910 Youtube: Coding TensorFlow
      </a>
    </p>

    <p>
      <a
        href="https://www.jetbrains.com/pycharm/learning-center/"
        target="_blank"
        >&#127910 Pycharm Learning Center
      </a>
    </p>

    <p>
      <a href="https://github.com/ageron/handson-ml2" target="_blank"
        >&#127760; Github: Hands-on Machine Learning with Scikit-Learn, Keras
        and TensorFlow
      </a>
    </p>


    <p>
      <a href="#" target="_blank">&#127760; Link</a>
    </p>
    <p>
      <a href="#" target="_blank">&#128229; Download</a>
    </p>
    <p>
      <a href="#" target="_blank">&#128212; Contents</a>
    </p>

    <div style="padding-top: 10em;"></div>

    <!-- Ending of Content -->
    <hr />
    <footer>
      &copy; Jingwora All rights reserved.
    </footer>

    <!-- Script -->
  </body>
</html>
