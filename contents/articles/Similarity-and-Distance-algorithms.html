<!DOCTYPE html>
<html>
  <head>
    <meta charset="UTF-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta
      name="description"
      content="Wrap-up DSimilarity and Distance algorithms"
    />
    <meta name="keywards" content="Similarity and Distance algorithms" />
    <meta name="author" content="Kritdikoon Woraitthinan" />
    <title>WRAP-UP: Similarity and Distance algorithms</title>
    <link rel="stylesheet" href="/contents_resources/styles/mainStyles.css" />
    <link rel="icon" type="image/png" href="/images/J_favicon/favicon.ico" />
  </head>

  <body>
    <!-- NAVNIGATION BAR -->
    <p style="padding: 20px;"></p>

    <div class="navbar">
      <a class="main" href="https://jingwora.github.io">
        <img src="/images/J_favicon/favicon.ico" width="15" />
      </a>
      <a href="https://jingwora.github.io/contents.html">◀ CONTENTS</a>
    </div>

    <!-- CONTENT -->

    <h1>
      <img src="/contents_resources/icons/wrap-up.png" width="80" />
      WRAP-UP: <br /><br />
      Similarity and Distance algorithms
    </h1>

    <!-- Table of Contents -->

    <h2>TOC</h2>
    <p><a href="#navh01">Overview▸</a></p>
    <p><a href="#navh02">Geometry-based distances▸</a></p>
    <p><a href="#navh03">Similarity-based distances▸</a></p>

    <p><a href="#navh_resources">Resources▸</a></p>
    <hr />

    <!-- Overview -->

    <div id="navh01"></div>
    <div style="padding-top: 2em;"></div>
    <h2>Overview</h2>
    <p><a href="#TOP">[TOP]</a></p>

    <p>
      In machine learning, measuring similarity and distnace of data is core in
      many machine learning tasks. In unspervised classification task, distance
      is used to assign classes. In recommendation engines, recommended list can
      be created from similarity of used contents (Content-Based
      Recommendations) or similarity of user profile (User-based Collaborative
      Filtering). In Natural Language Processing (NLP) task, key words in
      document is extracted based on similarity algorithm.
    </p>

    <p>
      By understanding similarity and distance algorithms, we can improve
      machine learning performance. This is my wrap-up of Similarity and
      distance algorithms.
    </p>
    <hr />

    <!-- Geometry-based distances -->

    <div id="navh02"></div>
    <div style="padding-top: 2em;"></div>
    <h2>Geometry-based distances</h2>
    <p><a href="#TOP">[TOP]</a></p>

    <p>
      Geometry-based distances measure of how far part points are based on
      geometrical distance.
    </p>

    <h3>Euclidean Distance</h3>
    <p>
      The Euclidean distance (or Euclidean metric) is a straight-line distance
      between two vectors.
    </p>
    <p>
      The Euclidean distance between points p and q is the length of the line
      segment connecting them.
    </p>
    <a href="./Similarity-and-Distance-algorithms/Euclidean-Distance_p.png">
      <img
        src="./Similarity-and-Distance-algorithms/Euclidean-Distance_p.png"
        width="250"
      />
    </a>

    <p>Formula:</p>
    <a href="./Similarity-and-Distance-algorithms/Euclidean-Distance.svg">
      <img src="./Similarity-and-Distance-algorithms/Euclidean-Distance.svg" />
    </a>

    <p>Python Script:</p>
    <textarea class="ta2">
    import scipy.spatial.distance as dist
    A = [(1, 1)]
    B = [(4, 5)]
    euclidean_distance = dist.euclidean(A, B)
    print("Euclidean distance is {:.4f}".format(euclidean_distance))
    >>> Euclidean distance is 5.0000
    </textarea>

    <h3>Manhattan Distance</h3>
    <p>
      The Manhattan Distance (or taxicab metric) is the sum of the horizontal
      and vertical distances between points on a grid.
    </p>
    <p>
      You can imagine this metric as a way to compute the distance between two
      points when you are not able to go through buildings.
    </p>
    <a href="./Similarity-and-Distance-algorithms/Manhattan-Distance_p.png">
      <img
        src="./Similarity-and-Distance-algorithms/Manhattan-Distance_p.png"
        width="250"
      />
    </a>

    <p>Formula:</p>
    <a href="./Similarity-and-Distance-algorithms/Manhattan-Distance.svg">
      <img src="./Similarity-and-Distance-algorithms/Manhattan-Distance.svg" />
    </a>

    <p>Python Script:</p>
    <textarea class="ta2">
    import scipy.spatial.distance as dist
    A = [(1, 1)]
    B = [(4, 5)]
    manhattan_distance = dist.cityblock(A, B)
    print("Manhattan distance is {:.4f}".format(manhattan_distance))
    >>> Manhattan distance is 7.0000
    </textarea>

    <h3>Minkowski Distance</h3>
    <p>
      Minkowski distance (or Minkowski metric) is a metric in a normed vector
      space which can be considered as a generalization of both the Euclidean
      distance and the Manhattan distance.
    </p>

    <a href="./Similarity-and-Distance-algorithms/Minkowski-Distance_p.png">
      <img
        src="./Similarity-and-Distance-algorithms/Minkowski-Distance_p.png"
        width="250"
      />
    </a>

    <p>Formula:</p>
    <a href="./Similarity-and-Distance-algorithms/Minkowski-Distance.svg">
      <img src="./Similarity-and-Distance-algorithms/Minkowski-Distance.svg" />
    </a>

    <p>Python Script:</p>
    <textarea class="ta3">
    import scipy.spatial.distance as dist
    A = [(1, 1)]
    B = [(4, 5)]
    minkowskin_distance = dist.minkowski(A, B, w=(0.5,1.5))
    # w = The weights for each value in A and B.
    print("Minkowski distance is {:.4f}".format(minkowskin_distance))
    >>> Minkowski distance is 5.3385
    </textarea>

    <h3>Chebyshev Distance</h3>
    <p>
      Chebyshev distance (or Tchebychev distance, maximum metric, chessboard
      distance) is a distance metric which is the maximum absolute distance in
      one dimension of two N dimensional points. It has real world applications
      in Chess, Warehouse logistics and many other fields.
    </p>

    <a href="./Similarity-and-Distance-algorithms/Chebyshev-Distance_p.png">
      <img
        src="./Similarity-and-Distance-algorithms/Chebyshev-Distance_p.png"
        width="250"
      />
    </a>

    <p>Formula:</p>
    <a href="./Similarity-and-Distance-algorithms/Chebyshev-Distance.svg">
      <img src="./Similarity-and-Distance-algorithms/Chebyshev-Distance.svg" />
    </a>

    <p>Python Script:</p>
    <textarea class="ta2">
    import scipy.spatial.distance as dist
    A = [(1, 1)]
    B = [(4, 5)]
    chebyshev_distance = dist.chebyshev(A, B)
    print("Chebyshev distance is {:.4f}".format(chebyshev_distance))
    >>> Chebyshev distance is 4.0000
    </textarea>

    <h3>Canberra Distance</h3>
    <p>
      The Canberra distance is a weighted version of Manhattan distance. It
      measures the sum of absolute fractional differences between the features
      of a pair of data points and is very sensitive to a small change when both
      coordinates are nearest to zero.
    </p>
    <p>
      The Canberra distance has been used as a metric for comparing ranked
      lists[3] and for intrusion detection in computer security.
    </p>

    <p>Formula:</p>
    <a href="./Similarity-and-Distance-algorithms/Canberra-Distance.svg">
      <img src="./Similarity-and-Distance-algorithms/Canberra-Distance.svg" />
    </a>

    <p>Python Script:</p>
    <textarea class="ta2">
    import scipy.spatial.distance as dist
    A = [(1, 1)]
    B = [(4, 5)]
    canberra_distance = dist.canberra(A, B)
    print("Canberra distance is {:.4f}".format(canberra_distance))
    >>> Canberra distance is 1.2667
    </textarea>

    <hr />
    <br />

    <!-- Similarity-based distances -->

    <div id="navh03"></div>
    <div style="padding-top: 2em;"></div>
    <h2>Similarity-based distances</h2>
    <p><a href="#TOP">[TOP]</a></p>

    <p>
      Similarity-based distances considers two objects to be similar based on
      variouse factors like correlation, cosine distance, etc.
    </p>

    <h3>Pearson correlation distance</h3>
    <p>
      Correlation is a technique for investigating the linear relationship
      between two continuous variables based on the Pearson correlation
      coefficient.
    </p>
    <p>
      The Pearson’s correlation ranges from -1 to +1. A value of 1 implies
      perfect positive relationship. A value of -1 implies perfect negative
      relationship. A value of 0 implies no linear relationship.
    </p>
    <p>
      correlation-based distance is suitable for when you want to identify
      clusters of observations with the same overall profiles.
    </p>
    <a href="./Similarity-and-Distance-algorithms/Pearsons-Correlation_p.png">
      <img
        src="./Similarity-and-Distance-algorithms/Pearsons-Correlation_p.png"
        width="250"
      />
    </a>

    <p>Formula:</p>
    <a href="./Similarity-and-Distance-algorithms/Pearsons-Correlation.png">
      <img
        src="./Similarity-and-Distance-algorithms/Pearsons-Correlation.png"
      />
    </a>

    <p>Python Script:</p>
    <textarea class="ta2">
    from scipy import stats
    A = [1, 2, 3, 4, 5]
    B = [0, 1, 4, 10, 15]
    pearson_correlation, _ = stats.pearsonr(A, B)
    print("Pearson's correlation is {:.4f}".format(pearson_correlation))
    >>> Pearson's correlation is 0.9690
    </textarea>

    <h3>Spearman’s correlation distance</h3>
    <p>
      Spearman's correlation is the nonparametric version of the Pearson
      product-moment correlation. It is calculated similarly to Pearson’s
      correlation but uses ranked data values. It determines the strength and
      direction of the monotonic relationship (whether linear or not).
    </p>
    <p>
      Spearman's correlation ranges between -1 and +1, where -1 implies strong
      negative relationship and 1 implies strong positive relationship.
    </p>
    <p>
      Spearman’s measure is more computationally efficient compared to Kendall’s
      Tau.
    </p>

    <a href="./Similarity-and-Distance-algorithms/Spearmans-Correlation_p.png">
      <img
        src="./Similarity-and-Distance-algorithms/Spearmans-Correlation_p.png"
        width="250"
      />
    </a>

    <p>Formula:</p>
    <a href="./Similarity-and-Distance-algorithms/Spearman’s Correlation.png">
      <img
        src="./Similarity-and-Distance-algorithms/Spearman’s Correlation.png"
      />
    </a>

    <p>Python Script:</p>
    <textarea class="ta2">
    from scipy import stats
    A = [1, 2, 3, 4, 5]
    B = [0, 1, 4, 10, 7]
    pearson_correlation, _ = stats.pearsonr(A, B)
    print("Pearson's correlation is {:.4f}".format(pearson_correlation))
    >>> Spearmans correlation is 0.8743
    </textarea>

    <h3>Kendall’s Tau correlation distance</h3>
    <p>
      Kendall tau rank distance (or bubble-sort distance) is a metric that
      counts the number of pairwise disagreements between two ranking lists. The
      larger the distance, the more dissimilar the two lists are.
    </p>
    <p>
      Kendall’s Tau ranges between -1 and +1 , where -1 suggests a strong,
      negative relationship between two variables and 1 suggests a strong,
      positive relationship between two variables.
    </p>
    <p>Kendall’s Tau has smaller variability when using larger sample sizes.</p>
    <a href="./Similarity-and-Distance-algorithms/Kendall-Correlation_p.png">
      <img
        src="./Similarity-and-Distance-algorithms/Kendall-Correlation_p.png"
        width="250"
      />
    </a>

    <p>Formula:</p>
    <a href="./Similarity-and-Distance-algorithms/Kendall-Correlation.png">
      <img src="./Similarity-and-Distance-algorithms/Kendall-Correlation.png" />
    </a>

    <p>Python Script:</p>
    <textarea class="ta2">
    from scipy import stats
    A = [1, 2, 3, 4, 5]
    B = [0, 1, 4, 10, 7]
    kendall_correlation, _ = stats.kendalltau(A, B)
    print("Kendall tau correlation is {:.4f}".format(kendall_correlation))
    >>> Kendall tau correlation is 0.8000
    </textarea>

    <h3>Cosine distances</h3>
    <p>
      The cosine distances calculates the cosine of the angle between two
      vectors.
    </p>
    <p>
      Two vectors with the same orientation have a cosine similarity of 1, two
      vectors oriented at 90° relative to each other have a similarity of 0, and
      two vectors diametrically opposed have a similarity of -1.
    </p>
    <a href="./Similarity-and-Distance-algorithms/Cosine_p.png">
      <img
        src="./Similarity-and-Distance-algorithms/Cosine_p.png"
        width="250"
      />
    </a>

    <p>Formula:</p>
    <a href="./Similarity-and-Distance-algorithms/Cosine.svg">
      <img src="./Similarity-and-Distance-algorithms/Cosine.svg" />
    </a>

    <p>Python Script:</p>
    <textarea class="ta2">
      import scipy.spatial.distance as dist
      A = [1, 2, 4, 5, 2]
      B = [1, 3, 5, 6, 2]
      cosine_distance = dist.cosine(A, B)
      print("Cosine distance is {:.4f}".format(cosine_distance))
      >>>Cosine distance is 0.0039
    </textarea>

    <h3>Jaccard Similarity</h3>
    <p>
      Cosine similarity is for comparing two real-valued vectors, but Jaccard
      similarity is for comparing two binary vectors (sets). Jaccard similarity
      divides the size of the intersection by the size of the union of the
      sample sets.
    </p>
    <p>
      It’s a measure of similarity for the two sets of data, with a range from 0
      to 1. The higher the value, the more similar the two sets.
    </p>
    <a href="./Similarity-and-Distance-algorithms/Jaccard-Similarity_p.png">
      <img
        src="./Similarity-and-Distance-algorithms/Jaccard-Similarity_p.png"
        width="250"
      />
    </a>

    <p>Formula:</p>
    <a href="./Similarity-and-Distance-algorithms/Jaccard-Similarity.png">
      <img src="./Similarity-and-Distance-algorithms/Jaccard-Similarity.png" />
    </a>

    <p>Python Script:</p>
    <textarea class="ta2">
    import scipy.spatial.distance as dist
    A = [1, 1, 1, 0, 0]
    B = [1, 1, 0, 1, 0]
    jaccard_distance = dist.jaccard(A, B)
    print("Jaccard distance is {:.4f}".format(jaccard_distance))
    >>> Jaccard distance is 0.5000
    </textarea>

    <hr />

    <div id="navh_resources"></div>
    <div style="padding-top: 2em;"></div>
    <h2>Resources</h2>
    <p><a href="#TOP">[TOP]</a></p>

    <p>
      <a
        href="https://docs.scipy.org/doc/scipy/reference/stats.html"
        target="_blank"
        >&#127760; SciPy Reference Guide: Scipy Statistical functions</a
      >
    </p>

    <p>
      <a
        href="https://docs.scipy.org/doc/scipy/reference/generated/scipy.spatial.distance.cdist.html"
        target="_blank"
        >&#127760; SciPy Reference Guide: Scipy Distance functions</a
      >
    </p>

    <div style="padding-top: 2em;"></div>

    <!-- Ending of Content -->
    <hr />
    <footer>
      &copy; Jingwora All rights reserved.
    </footer>

    <!-- Script -->
  </body>
</html>
